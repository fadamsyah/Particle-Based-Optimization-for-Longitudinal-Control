{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T18:05:48.026812Z",
     "start_time": "2020-05-14T18:05:43.458330Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numba as nb\n",
    "import scipy as sc\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "from scipy.integrate import cumtrapz\n",
    "from numba import jit, njit, prange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD PARAMETER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steady State Response\n",
    "param_ssr = np.load('../model/ssr.npy')[-1]\n",
    "\n",
    "# Dynamics\n",
    "param_dynamics = np.load('../model/sys_id.npy')[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step & ramp function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T18:05:48.089304Z",
     "start_time": "2020-05-14T18:05:48.062885Z"
    }
   },
   "outputs": [],
   "source": [
    "def step(tt):\n",
    "    out = np.zeros_like(tt)\n",
    "    out[tt >= 0] = 1\n",
    "    return out\n",
    "\n",
    "def ramp(tt):\n",
    "    out = np.array(tt)\n",
    "    out[tt < 0] = 0\n",
    "    return out\n",
    "\n",
    "def jitter(gain, omega, tt, t0, tf):\n",
    "    out = np.array(tt)\n",
    "    \n",
    "    out = gain * np.sin(omega*(tt-t0))\n",
    "    out[tt-t0 < 0] = 0\n",
    "    out[tt-tf > 0] = 0\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T18:05:49.857464Z",
     "start_time": "2020-05-14T18:05:48.546233Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAasAAAEkCAYAAACR9x5gAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABqWUlEQVR4nO2deXhU5fX4P2eybyxhCTsBZN9EEdxxhxpFxbUu1WqL1tZdK2pbsf1pY6ttXapfqbtQd1E0iqIiFsQFZRcQCAECCQRIyL5M5vz+uHMnkz2TzGRmkvfzPHky99533nvuTHLPPct7jqgqBoPBYDCEMo5gC2AwGAwGQ3MYZWUwGAyGkMcoK4PBYDCEPEZZGQwGgyHkiQy2AAaDwRAqfP/9970jIyOfBcZhHuaDhQvY4HQ6f3X00Ufvt3caZWUwGAxuIiMjn+3Tp8/oXr165TscDpMqHQRcLpfk5eWNyc3NfRaYae83Tw4Gg8FQw7hevXoVGkUVPBwOh/bq1eswlnVbsz9I8hgMBkMo4jCKKvi4v4Na+skoK4PBYAgx7r777j5HHHHE2BEjRowZNWrUmM8//zwhUOeaMmXKyC+//DI+UPP7CxOzMhgMhhDi008/Tfj444+7rV+//se4uDjNycmJrKiokGDLFWyMZWUwGAwhxJ49e6KSk5OdcXFxCtC3b19nampqVf/+/cf/5je/6T9+/PjR48ePH71hw4YYgL1790ZOnz592Lhx40aPGzdu9CeffJIAUFhY6Lj44otTx40bN3r06NFj5s+f3w2guLhYzjnnnKEjRowYk5aWNrS8vDwsFKGxrAwGg6EBUudkHB2IebPS075v6vj5559f+Ne//rVfamrquBNPPLHw5z//+aG0tLRigC5dulSvX79+05NPPtnjpptuGrh06dJt119//cDbb7993/Tp04u3bt0aPX369OGZmZkb77333r6nnnpq4Ztvvpl14MCBiMmTJ4+eOXNm4T/+8Y9ecXFxrp9++unHb775Ju6EE04YE4jr9DdGWRkMBkMI0bVrV9eGDRt+XLx4cdJnn32WdPXVVw/705/+lA1w9dVXHwL49a9/fegPf/jDQIAVK1Z02bp1a5z9/uLi4oj8/HzHF1980eXjjz/u9vjjj/cBqKiokG3btkUvX7488eabb94PMHXq1LIRI0aUtv9V+o5RVgaDwdAAzVlAgSQyMpJzzjmn6JxzzimaMGFC2SuvvNIDwOGoidyIiAKoKqtWrdqUmJhYK4tRVXnrrbe2TZw4saLu/CJh4fmrhYlZGQwGQwixdu3amPXr18fY26tXr44bMGBAJcDLL7+cDPDcc891nzRpUgnAiSeeWPjwww/3tsd/9dVXcQCnnnpq4aOPPpricrkAWLFiRZx7fPH8+fOTAb777rvYn376KeQzAcFYVgaDwRBSFBYWRtx8882DCgsLIyIiIjQ1NbXipZde2jl58uSuFRUVMmHChFEul0tee+21TIB58+bt/tWvfjVoxIgRY6qrq2Xq1KlFxx9//K709PS9s2fPHjRq1KgxqioDBgyoWLp06bY777xz/2WXXTZkxIgRY8aOHVs6fvz4kmBfc0sQ03zRYDAYLNauXZs1ceLEA8GWoyH69+8/ftWqVZv69u3rDLYs7cHatWt7Tpw4MdXeNm5Ag8FgMIQ8xg1oMBgMYcCePXvWB1uGYGIsK4PBYDCEPEZZGUISEUkRkS9FpEhEHg22PMFERO4VkWeDLYfBEEyMsjIEDBHJEpEyESkWkVwReVFEElv49tnAAaCLqt4RQDEDgoh85L7uYhGpEpFKr+3/82UuVX1IVX/VRnlOEZHstsxhMAQTo6wMgeZcVU0EjgQmAfe08H2DgR+1FemqIhL0WKyq/kxVE93XvgD4m72tqjfY40JB1pYQLnIaOi5GWRnaBVXNBT7GUloAiMixIvKViBSIyFoROcW9/0XgauD3bkvkDBFxiMgcEdkuIgdF5A0RSXaPTxURFZHrRGQX8Ll7/7UisklE8kXkYxEZ7HVuFZEbRGSr+/i/xWtZv4j82v3eIhH5UUSOcu/vJyJvi0ieiOwQkZt9/Szc5/6tiGwFtrr3PSYiu0WkUES+F5GTvMbPFZH5zX1u7mPJIvKCiOx1X9e7IpIAfAT087Lu+olIjIj8yz12r/t1jHueU0QkW0TuFpFc4AUR2SAi53qdK0pEDoiI5zs1+IeXX365m4gcvXr16lh/zblly5bo4cOHj/XXfHPmzOnjvT1p0qRR/pq7IYyyMrQLIjIA+Bmwzb3dH8gA/h+QDNwJvC0ivVT1GmpbI58CNwPnA9OAfkA+8O86p5kGjAami8j5wL3ALKAX8D/g1TrjzwGOASYClwDT3bJdDMwFfgF0wWqtfVBEHMD7wFqgP3A6cKuITG/FR3I+MBWwi4h+h6XIk4H/Am+KSL0bVVOfm3vIK0A8MBboDfxTVUuwPvu9XtbdXuA+4Fj3eScCU4A/eJ2uj/scg7Hcsi8DV3odPxvIUdU1rbh+QxO89tpryUcddVTxK6+8khwsGZzOppdzPf744329t1evXr05kPIYZWUINO+KSBGwG9gP3O/efyXwoap+qKouVV0CrMK6ATbE9cB9qpqtqhVYyuSiOu6puapaoqpl7vF/VdVNquoEHgKO9LaugHRVLVDVXcBSaqy+X2Epyu/UYpuq7sRSbL1U9c+qWqmqmcB/gMta8bn8VVUPuWVFVeer6kFVdarqo0AMMLKB9zX6uYlIXyyldIOq5qtqlaoua0KGK4A/q+p+Vc0DHgCu8jruAu5X1Qq3nPPd5+niPn4VlnI0+JHDhw87Vq1alfjCCy9kLVy4sDtYimP27NkDRowYMWbEiBFjHnzwwd4Ay5Yti580adKokSNHjhk/fvzo/Px8h9Pp5Prrrx8wbty40SNGjBjz97//vWfdczQ25oMPPkiaOnXqiHPPPXfIyJEjxwKcccYZw8aOHTv6iCOOGPvII4/0BLjxxhv7V1RUOEaNGjVm5syZQwDi4+MnAbhcLq6//voBw4cPHztixIgx//nPf7rbc0+ZMmXkjBkzhg4ZMmTszJkzh9iloFqC8UMbAs35qvqpiEzDshh6AgVYT+sXe7uVgCgspdEQg4GFIuL9110NpHht764z/rE6mYSCZRHtdG/neh0rBezkj4HA9kZk6CciBV77IrCsNl/xlhURuQNLSfYDFMuiq3eToenPbSBwSFXzWyhDP2o+C9yv+3lt56lqub2hqntFZAVwoYgsxFKMt7TwXGHJlClT6j0wzJo169CcOXPyioqKHKeffvrwusevvPLKAzfffPPBnJycyPPOO2+Y97Fvv/12S3PnXLBgQbdTTjnl8IQJEyq6detWvXz58vgVK1Yk7Ny5M2bjxo0/RkVFsW/fvojy8nK54oorhi1YsGD7tGnTSg8dOuRITEx0/etf/+rZtWvX6g0bNmwqKyuTY445ZtS5555b6OXlprExAOvWrUtYvXr1xlGjRlW65clKSUmpLi4ulkmTJo258sor85966qk9L774Yu/Nmzf/WFf+l19+udv69evjNm3atDEnJydyypQpo88666xigE2bNsWtWbMmMzU1teroo48etWTJksTp06cXt+CrMMrK0D6o6jJ3LOoRLBfYbuAVVf11C6fYDVyrqivqHhCRVPs0dcY/qKoLWiHubmBYI/t3qGq9G1Qr8Mjqjk/djeVW3KiqLhHJx1KuDcnQ4OfmtqySRaSbqhY0dj4v9mIpv43u7UHufU295yUspRoJrFTVPQ2MMbSBN954I/mWW27ZD3DhhRceeuWVV5KzsrJibrjhhryoqCgAUlJSqr/99tu43r17V02bNq0UIDk52QXw6aefdtm8eXP8okWLugMUFRVF/Pjjj7Fjx471PHg0NiY6OlonTJhQYisqgIcffjglIyOjG0Bubm7Uxo0bY/v06dNoPcH//e9/SZdccsmhyMhIBg4c6Jw6dWrx8uXL47t27eoaP358ybBhw6oAxo4dW7p9+/boln4uRlkZ2pN/AVnugPx84Dt3vOdTLOvgWGCbqjaUYv1/wIMicrWq7nTHaI5X1fcaOdf/AX8RkTWqulFEugJnqeqbLZDzWeAfIrIc+AFLcVUB3wKFInI38DhQiRUji1PV71ryATRCEuAE8oBIEZmDZVk1RJOfm4h8BDwlIr8FioHjVPVLYB/QQ0S6quph91yvAn8Qke+wFNOf3PM3xbvAU1gW7d9adbVhRFOWUFJSkqup43379nW2xJLyJjc3N+Lrr7/u8tNPP8X97ne/o7q6WkREx40bVyruliA2qkrdfe798uijj+668MILC733b9myJbq5MR988EFSfHy8y3t72bJlSatWrdqclJTkmjJlysiysrImw0dNJfDGxMR4DkZEROB0Olvcq8TErAzthjsu8jLwR1XdDZyHlQSRh2Ux3EXjf5OPAYuAT9wxsK+xEhQaO9dC4GHgNREpBDZgua1aIuebwINYbssirBt0sqpWA+dixbZ2YK0Dexbo2pJ5m+BjrGy9n7BcceXUcRN6ydbc53YVlmLdjBUjvNX9vs1YyinTnUXYDytJYxWwDliPpZj/X1OCumNXbwNDgHdac7GGxnnllVe6z5o16+DevXvX79mzZ31ubu66AQMGVB555JGl//d//9erqqoKgH379kVMnDixfN++fdHLli2LB8jPz3dUVVVx5plnHn766ad7VVRUCMC6detiCgsLa/1ftWQMQEFBQUTXrl2rk5KSXKtXr45du3Ztgn0sMjJS7fd7M23atKK33nor2el0snfv3shvv/028aSTTmpzZXdjWRkChqqmNrDvN16vv8HK4GvovdfU2XYB/3D/1B2bRQMuM1V9hUYSAFRV6mzXPd//YVlndd+3F/h5Q3M2RgNz1z13NXCd+8fG22pxYFlx9vimPrdDWGn/DR27toHdN7t/6o79AhjQ0DzALmChqrYo1mBoOW+++WaP3//+9zne+84777z8TZs2xQ4YMKBy1KhRYyMjI/Xqq6/Ou/fee/MWLFiw/eabbx5UXl7uiI2NdX355Zc/3XbbbQeysrJixo8fP1pVJTk5uerDDz+sFYNtyRiACy+88PC8efN6jRgxYsywYcPKJ06c6FE6V1xxRd7o0aPHjBs3rnTRokU77P1XXXVVwVdffZU4evTosSKiDzzwQPagQYOc69ata9NnY1qEGAwhjHvt15vAWlX9SwjIkwysBq5yuxc7FKHcIqSzYVqEGAzhxQ9YFs5/gi2IiPway+34UUdUVIbQxrgBDYYQRlUnBVsGG1X9DyGgNA2dE2NZGQwGgyHkCUvLyuFwaFxcXLDFMBgMHYy33noLp9M52HsBbUfC5XIxefLk74MtR3O4XC7BqqDiISyVVVxcHCUlbc6ENBgMhlrs2LGDpKQkevToQUdUWN9//33L6xsFCZfLJXl5eV2xlpt4CEtlZTAYDIFgwIABZGdnk5eXF2xRAsKBAwdk7dq1DZXxCiVcwAan01mrh1tYpq4nJCSosawMBoPBN0SkVFUTmh8ZepgEC4PBYDCEPEZZGQwGgyHkMcrKYDAYDCGPUVYGg8FgCHmMsjIYDAZDyGNS1w0GQ1jhrHZx78L1ZB0sxSEQ4RAcYv1Yr6m13TU+it9PH0m3+Bb3+TOEIEZZGQwhykfrc/h4Yy4AIoIICNZvh9frmmPWTdp+be/vnRTLr08aQmREx3CkrNqZzxurGurP2Tj9u8Xx21OPCJBEhvbAKCuDIQQpr6rmjjfXUlpZ7Zf5BiXHkzahr1/mCjYb9liNjqePTeHq41KpVsWl4HIpLlWqXe5tVd5ctZulW/LYklsUZKkNbcUoK4MhBNl1qNSjqP5xyURUrb7zLvcLxbohq9drVK0xLuu3Kjz++VYKSqvYsq+INDqGstq41+rEfvKIXhx/RNPFGHolxbB0Sx67DpW2h2gdAhHphtUBexzWn921qroyqEJhlJXBEJLsL6wA4LihPZh1VGMNe5snNiqCexeuZ29Bmb9ECzob91qW1bh+XZsdm5IUC0BeUUVAZepgPAYsVtWLRCQaiA+2QGCUlcEQkuwrLAegd5eYNs3TO8l6/8HijnGzLqusZtv+YiIcwsg+Sc2O754QBUBBaWWgResQiEgX4GTgGgBVrQRC4sPrGBFXg6GDkedWLr0S26asusZbN+vDZVVtlikU2JRbiEtheO9EYqMimh2fGBNJVIRQUllNeZV/4n9hTqSIrPL6mV3n+FAgD3hBRFaLyLMiEhK1BI2yMhhCkJIKJwBd4qLaNE9X9/sLy51tlikUsONVY/p1adF4EaG7O2W9oLRjKOw24lTVyV4/8+ocjwSOAp52d6kuAea0u5QNYJSVwRCC2MkVcS2wHpqiS2zHsqw27ml5vMqmm9u6LCgLCW9WqJMNZKvqN+7tt7CUV9AxyspgCEHK3C6r2Oi2KSvbsuowysptWY3r33JllRhjheZta9XQOKqaC+wWkZHuXacDPwZRJA8mwcJgCEHK/WRZxUY5iHAIlU4XVdUuosJ4YXBVtcuzXmp03+aTK2wS3MqqqIO4QtuBm4AF7kzATOCXQZYHMMrKYAhJbMsqvo2WlYgQHxVBUYWTsqrqsFZWW/cVU1ntYkjPBJJiWx7LS4q1LSuTYNESVHUNMDnYctQlfP9yDYYOjL9iVlDjSizzUzWMYLHBvb6qpckVNgnRlrIqrugYrtDOilFWBkMI4olZ+UFZ2daZv0o3BYsf7XiVD8kVUOMGLDaWVVjTbspKRJ4Xkf0isqGBY3eKiIpI07VTDIZOQrmf3IBQY52FvWXlzgQc66NlZbsBi03MKqxpT8vqRWBG3Z0iMhA4E9jVjrIYDCGNrVji/KGsbDdgVfjerKtdyo85lmXlq7KqsayMGzCcaTdlpapfAocaOPRP4PdYBRMNBgP+jVl1BDdg1sESSiur6ds1lh4+VvVING7ADkFQY1YiMhPYo6prgymHwRBqlPsxZhUXZd2sw9kNWOMC9C1eBd7KKnwtS0MQU9dFJB64DzirheNnA7MBoqNNx09Dx8Zfqevg7QYMX2VlJ1f46gKEGjegWRQc3gTTshoGDAHWikgWMAD4QUT6NDRYVefZ9awiI83yMEPHRVX9mw3YARIs7LR1XypX2HgsK5NgEdYE7a6vquuB3va2W2FNVtUDwZLJYAgFKpwuVCE60qo+0VbiwjxmpapeZZZ8t6xsZVVkLKuwpj1T118FVgIjRSRbRK5rr3MbDOFEmR+TKyD83YB7CsooKK0iOSGaPl1ifX5/YqxxA3YE2s2yUtWfN3M8tZ1EMRhCGn/GqyD811lt9IpXifhuaZoEi46BqWBhMIQYtrLyl2UVH+aW1cY2ZAKC16Jgo6zCGqOsDIYQw7aA/JFc4T1PuCqrDW2IVwHERNZUnq9whudnYDDKymAIOQLlBiwPWzdg2ywrEfHqaRWen4HBKCuDIeTwZ6kl73nCMRswr6iCfYUVJMZEMjg5vtXzmPT18McsWDIYQoxSP7sBwzkb0LaqxvTtgqMNafwmbtVy3MuIioBqwKmqIdHbyigrgyHEKPdzgkVcGMesPJmArYxX2SSYjEBfOTXU1rwaN6DBEGIELGYVlsrKXbmilfEqm0RTeT3sMcrKYAgx/J0NGM5V1zfs8Y9llRhrKq/7gAKfiMj37pqsIYFxAxoMIYZnnZWfLKvYMF0UfLisil2HSomJdHBEr8Q2zZVkEixsIkVkldf2PFWdV2fMCaq6V0R6A0tEZLO7xVNQMcrKYAgxAlVuKdzcgHal9VF9koiMaJsTyLgBPTSbMKGqe92/94vIQmAKEHRlZdyABkOIEaiYVbi5AT3rq1pRab0uCcayahEikiAiSfZrrBZOG4IrlYWxrMKIldsPkrF+Ly4FARwiOAQcDmn4tVivh/RM4PxJ/YMtvqGF+LM9CNTOBlTVVtXXCwYb29DDqi5JJmbVUlKAhe6/kUjgv6q6OLgiWRhlFSYcLK7g6he+pdLpatX7BybHcfTgZD9LZQgE/nYDOhxCTKSDCqeLCqfLb0ow0NjdgduaCQjGDdhSVDUTmBhsORrCKKsw4bNN+6l0uujfLY7fnDIMxerzU+1SXFr7tUvVvQ3//mIblU4X3+7IN8oqTPB3BQt7rgqni7LK6rBQVmWV1WzPKybCIYzsk9Tm+RLNouCwxyirMGFtdgEAvzwhlSuPHdzi9/XuEsM976xn676iAElm8Df+zgYEy0oroIrSqmq6+23WwLEptxCXwqiURL8oVztmVWRiVmGLSbAIE35yKxtfnzIHdI8DIOdwud9lMgQGf7cIAa+SS2GSZFETr2q7CxBqUtdNA8bwxSirMEBV2ZLrVlYpvikru7PqvkKjrMIFf8esvOcKl/T1mh5WbU+uAOMG7AgYZRUGFFU4KSx3EhcVQa+kGJ/em9LVUlY5h8tR1UCIZ/Az/k5dh/CrD7jR08PKP5aVqboe/hhlFQbsc7vw+nSN9TntOCkmkphIB2VV1ZRXtS6T0NC++LvcEoRXm5CqapfHkzC6b9uTK6BGWRUZyypsaTdlJSLPi8h+Edngte/vIrJZRNaJyEIR6dZe8oQTdrzJdun5gojQNS4KgIKySr/KZQgM5QFKsIDwiFlt3VdMZbWLIT0TSIqN8sucCV4xK+NhCE/a07J6EZhRZ98SYJyqTgB+Au5pR3nChtzCGsuqNXSLt/7hD5eZNSbhQGkgYlZhVHJpg93Dyk/xKoCoCAexUQ5cGj6uUENt2k1ZuQshHqqz7xNVte3yr4EB7SVPOGG7AVNaYVkBNZZVqVFWoY6qBiYbMIxiVhv9uBjYm8QY6//AxK3Ck1CKWV0LfBRsIUIR27Lq20rLyiir8KHCXaEkJtLRps64dYkNo/qA/iyz5E1ijPUZmLhVeBISi4JF5D7ACSxoYsxsYDZAdHR0O0kWGhwstmJNPRN9ywS06RpnfV6Fxg0Y8gSiegXUZBaGuhuw2qX8mBMgZRVr1lqFM0FXViJyNXAOcLo2Efl091yZB5CQkNCpIqT5pZay6h7fumCzSbAIH0oD4AL0ni/UEyyyDpZQWllNv66x9Gjlw1ljmPT18CaoykpEZgB3A9NUtTSYsoQytvuuayuVlafitPknDXkCZVmFS+q6Xbx2jJ/jVVATszJuwPCkPVPXXwVWAiNFJFtErgOeBJKwulGuEZH/ay95wgnbIuoe3zr3Z4LbV18S4jcqg1faup8tq9gwSbD40bMY2L8uQKiJWZmHtvCk3SwrVf15A7ufa6/zhyuqSr7bsmq9srK+5tJK808a6gQibR3CJ2Zlp637qyagN56Ylfk/CEtCKRvQ0ABlVdVUOl1ER1rrRFpDQrQdWA7tG1VL2XWwlCc/38q2/R2vknwgKq5DaMas3vkhm68zD3q2VdWrzFIgLCu3G9BYVmGJUVYhTo1VFdXqDq/xnnhF+P+TVlW7uPzZr3nkk5+44Kmv2FNQFmyR/EogitgCxNp/AyFiWWXmFXP7G2u5bN7XVFVb6fp7CsooKK0iOSG6VdVamiPJFLNtESISISKrReSDYMvijVFWIU5BadviVeBdaiY0blRt4evMg2TnWwqqqNzJk59vDbJE/iUQpZYA4u2q6yFiWa3LPux5vTnHspA37KlJWW/tg1lTJJo2IS3lFmCTvycVkQQRafUftlFWIY4nEzCu9TXSOpJl9eVPeQDMGNsHgPfX5oR8HMYXAhWz8vSzCpHPys76A1jvfv1jAONVUPPQZhIsGkdEBgBpwLN+mMshIpeLSIaI7Ac2AzkistFdF3a4L/MZZRXi5PvBsvKsL+kAT5R2TOOiowcwvn9Xiiuc/G/rgSBL5T8CHrMKFWW111tZFbj3BS5eBabyegv5F/B7wB8tGpYCw7BqvvZR1YGq2hs4Cau8XrqIXNnSyYyyCnHs4rPdWrnGCiDekw0YGjeq1qKqbHJXNxjdrwunjuwFUCtIH+4EPHU9BP4GXC5lo9vlBzWW1cYAW1ZmvSEAkSKyyutntn1ARM4B9qvq93461xmq+hdVXaeqHuWnqodU9W1VvRB4vaWTGWUV4pS640y2C6M1JLif0sPdV59XVEF+aRVdYiPp1zWWY4f2AGDl9o6jrAKVYBEfQm7A3fmlFFU4iY+OQAS25Baxp6CMfYUVJMZEMjg5PiDn9cSsOoA7vA04VXWy1888r2MnADNFJAt4DThNROa35iQicibwlIhMdG/Pbmicqra4BpxRViGO/Y+V0Aa3UHx0jWUVzr18dh6yipwM6ZmAiHDU4O5EOITNuYUhYTH4g9IAV7AIhc/JTqSYOiSZYb0SqapW3v4+G7DagvizgK83JmbVNKp6j6oOUNVU4DLgc1VtsZuuDjcCdwFXichpwJFtlc8oqxCn5ubVessqOtJBdIQDp0s9Vb3DkT3uLMD+3eMAy7U1vHciLoXNuYVNvTVsCFTMKjayxrJyuYL7wGLHq8b178p4d9v6177dBfi/eK03thvQxKzahTxVLVDVO4GzgGPaOqFRViGO7bqzSya1lviY0Hmybi32mqr+3eI8+8b0tW5uduJFuBOomJXDIcREWv/uwX5gsTMBx/arUVZ73T3b/N3DyhtbWRWWVYW1h6E9UNUvVPWcNkyR4TXXHODltspklFWIYyuX+DZYVlCzziac/fX2+qpayqpfx1JWgYpZQWjErepWqRg/oLZyGhugTECw/oe6xkVR4XRxsMR0IAgkqvpene0n2jpn0FuEGJrGVi7xbXQLdYSMQI9l1b0mAD/abVl1lNJLpQFyA4KlAPOpCqqyyjlczqGSSrrFR9G/W1y9JRlH9EoM6PkHJsdxeE8V/9uax1GDugf0XIGiS2wU3RPCp6efiCwDzlXVQhG5AYgFnlJVn54YjLIKcUo9llXbbl4JYdIioilyGnADDumZAMCOAyVBkcnflAfQsor1JFkEz7re4NWyXkTqZblGRgTW2TOsVyIb9hRy2+trA3qeQHLDtGHM+dmoYIvhC93ciupo4NfAB8B/gKt9mcQoqxDHVi5tSV0Hr4zAMA4u5xVXANC7S01Tvj5dYomLiuBAcSWHy6raVOkjFAhUggV4F7MNXszKXvjr7e47c0wKS37cxwlH9Aj4+WefPJQtuUVh/dDWljWXQaJKRCKBXwAPq+obIrLK10mMsgpx7ASLNrsB7bVWYfpPWlXtoqC0CpHa1TwcDiG1ZwKbcgrJOlDCxIHdgiekHygLUIIFhEbMaqOXZWXz++kj6ZkYw+9OOyLg5x/bryuLbz054Ocx1OIJYC2W+2+Oe5/P/l6TYBHilPorwSLMe1rluwPiyfHRRNRZhzO0A7kC7QSL2EC4AUOg5JJ32rrN8JQk/jprfC33riH8EZHjRERU9SVgKjBOVctE5AisRrw+YZRViFPqh0XB3u8PV/fHgWJLWfVMjKl3zI5bZeYVt6tMgcBWJG21pBuixg0YnAeWvKKKgFepMIQUVwPfi8hrwEVAVwBV3aaqv/R1MuMGDHE8llUbY1ZxYV5y6WCJFa/qkVg/CyrVtqwOlrarTIHAfjgJSMwqyG5Au/ZfIKtUGEIHVb0BQERGAT8DXhSRrlgFbhcDK1S1xX+MxrIKYVwu9VvLiITo8E5dP+i2rHo0aFlZT+lZYe4GdLmU8ior+cGuOOFPPDGrICVYeNZXBXDhryH0UNXNqvpPVZ0BnAYsBy4GvvFlnnZTViLyvIjsF5ENXvuSRWSJiGx1/w7PhQ8Bwn4Cjo1y1IvT+IpdwSJcFwUfcGcC9mhgfUlqD8uyyjpQEtaVCbyTKwJhedgxq2DFLT1p6wFc+GsITUQkCkBVy1T1Q1W9SVUn+zKHz8oqdU5GQuqcjNY89r0IzKizbw7wmaoOBz6jJlPEgFfaehuTK7znCNdyS3bFgZ4NuAGTE6JJio2kqMIZ1pUJApm2DjXWebCaVTaUXGHo+IjIs8A+EdktIt+IyH9E5CZf52n2Lpg6J8OBVYH3CqxihBVATOqcjDzgQ2BeVnpas73FVfVLEUmts/s84BT365eAL4C7Wyh7h8d+Ao5vY11A8I5ZhaeyOlBkWVYNJViICEN6JrAu+zBZB0oaHBMOBLLUkve8wYhZHS6tYvehMmKjHJ7sTUOn4SQgRVWrRKQ/MBGY4OskLbGsanV7zEpPG5iVnlar22PqnIzWlpFPUdUcAPfv3q2cp0NiK5b4KP9ZVuGaun7ITl1vpMyM7QoM5/T1QLUHsYkLYszKTq4Y3bdLwKtUGEKOr4HuAKq6x+0GTPd1kpbcBc/ISk+r1yArKz3tEPA28HbqnIyAL6l2N++aDRAdHT51sdpCWZX/LCt7jrBNsCixEywaUVYdYK1VINPWwTsbsP0fWDwuQJNc0RmZBywTkeewkirWqephXydpVlnZiip1TsYy4Nys9LTC1DkZnmKEWelplQ0psxayT0T6qmqOiPQF9jc20N3Rch5AQkJC+EbRfcC2rPwZswp/y6phF58nI/Bg+CorT9p6oN2AQXhgsRsumuSKTsl8rBYhkVhNGSeISKyqDvNlEl/ugt3ciqpNxQjrsMj9/nT37/eaHt65KPVTxXXvOcI1ZnXIq4JFQwzpaVVvycwLX2VVFmA3YDDLLdmW1VhjWXVGslX1fu8dIuJzYNkX53FV6pwMTzHCrPS0+4GxLX2ziLyKVWJjpIhki8h1WErqTBHZCpzp3ja48cSs/KiswtGyqnBWU1zhJNIhdIlr+PlqeO9ERGB7XjEVzvBUyP6qsN8YNeWW2jdmVVzhZMeBEqIihBEpSe16bkPLEZFYEflWRNaKyEYRecBPU68RkVu8d6hqha+T+GJZtakYoar+vJFDp/sgQ6fC7m3U1uoVUFO1PRxjVvkllpe5e0I0Ig2vP0qIiWRozwS255WwJbeICQO6taOE/qHGDRiYwjLBKre0KacQVRjZJ4noSJNcEcJUAKeparF7XdRyEflIVb9u47wpwBkicjfwA5YeWaOqb/oySbN/OalzMo5LnZMhWelpnmKEWelpZalzMlpVjNDQcux2Hm2tCwjhU27p2x2H6lWisEstNeYCtLEV1Lpsn2O3IcHhMkspB6rNSbDKLW1ooNK6IfRQC7vAZpT7p835Aap6iaqOBoYAfwJ+wtIlPtGSx5yrge9T52TUKkaYlZ62LSs9zedihIaWU+KniusAidGRiFhzOquD18+oKV79dheXPLOSc59c7qmyDs2nrdvYi03X+0lZvbt6Dy+vzKLaZf2/rtldwD+X/OSRLa+ogkc/2cKP7jJC1S7l2f9l8tH6HM8cn2zM5T9fZrbINVlYZj1IBEpZ1ZRbam9lZfewMsoqBIgUkVVeP7O9D4pIhIiswUp2W6KqPpVEqjNXLTeIqlao6g+q+pKq3tnQmCYFb25AVnraDQCpczI8xQhT52TUKkaYlZ4Wfr6lMKDMjwkWDofQLS6K/NIqCsqqQnLh7AsrdgBQVO7k4425XDZlEOClrBpJW7c5erBVrWvF9gOoaqMuw5bwdeZBbn19DQCqcPHkAfziuW8oLHeyce9hnr36GOa8vY7PNu9n/tc7WTHnNF7/bjf/L2MTAG9cfxzd4qO4fv73qEJheRV3nDWyyXPWWFYBcgO6H3qK29m63uhJrjCZgCGAs6kyR+7CskeKSDdgoYiMU9UNjY1vhqUi8jbwnqrusneKSDRwIpYhtBSrulGztNiBnJWetjkrPe2fWelpbSpGaGg5JX6quG7T3W2Z5IdgSaI9BWX8tK+mxce3WYc8r21l1VBdQG/G9+9K9/gosvPL2rze6s1V2Z7XL6/M4uONuRSWWzf5zzbv54dd+SzdYq20yC+t4oO1ObxR5z1vfZ+NXarw1W93UdWMRVtY7lZWAeoEa39+B4srcbnaZ/VHeVU1W/cX4xAY3ccoq3BBVQuwKgrVLZHnCzOAauBVEdkrIj+KSCawFfg58E9VfbGlk/l0F0ydkxGVlZ5WlZWeVoZVaulDX95v8A1/xqzA7rBb4rn5hxLrdhcAWDX+yp1s9VJc+wrd7eyTmrYGIxzCScN7sWjtXj7akMtvT21d59myymoWb6hx5W3PK2Huoh8926rwyxe+w/t+/9BHmygorVlu+PHGXKK8KjUcKK5k2ZY8zhiT0uh57WK93ZuJzbWW2KgIusZFcbisivzSygYr2PubzblFVLuUESmJAUvJN/gHEekFVKlqgYjEAWcAD7d2PlUtB54CnnInbPQEytyK0GdarKxS52Q8C8xKnZNRAuwF1gHrstLTnmjNiQ3N4+9UZvsmmF/a2jXcgWPLviIAzh7Xl9dX7SYzr9jjyss9XAZASpfYZue54Kj+LFq7l1dW7mTaiF7ERkWgqrgUXKq4VFHPazzb3mM+Wp9DSWU1Ewd2Y/Lg7jy3fAeHy6qIdAh/v3gCt72+1uOy+/tFE/jjexs8iurnUwax82AJX20/SFV1NQOT47jsmEH8/eMt/Od/mQzuEU+1Ki6Xda5ql3rkWrHtIEBAO+b2TorhcFkVKzMPMqqPlUauakXRrd/qsQbrbtfdZ73HOmi/H69jAAu+3gmY5IowoS/wkohEYHnd3lDVD/wxsapWATnNDmwCXyyrk4CUrPS0qtQ5Ga0uRmhoOf5qaW+TnGC5l/JLQ8+y2pJrKatjhyXz8Y+5FJRWsb+ogpQuseQWlgPQp2vzymra8F6M7deFjXsLOeeJ5W2S6aKj+nP04GSeW27F0k4d1ZtzJ/TjwYzNHCiuICE6gnMn9mPFtgO8u2YvABce1Z+dB0v5aruleC6YNIBZR/XnkU+28M2OQ5z5zy+bPGeEQ+jfPXDKakjPBLbuL+Z3/10dsHM0xPgBRlmFOqq6DpgUbDkaw5e7oF2McH9WetoeYA/GDRhQ7N5TCX6oDQg1llUougG3u1vSD++dRGqPBNaUFrDzYCkpXWI9bsC+LVBWDofwf1cezX3vbmD3oVJEwCGCw/1bvF47BKizbR8fnJzAJccMJCYygptPO4KvdxziD2mjiYxw8NdZ43ly6TZ+e8owYqMiuP3Mkew9XM6U1GQmpyYzYUA3Vmw7QFGFk1+fNISk2CjuO3s0r323G1UlwiHu84n12mGdM0KEs8am+O3hpCFmnzyUXYdKqXS6QMBOQRER3B8HguCdm+J9jDpjhJoDNe+veQ9YWZznH9k/YNdk6Bz48l8xD1iWOifDU4wwKz0tPBe0hAmlFf61rOwMwP1uSyVUUFX25FuuvoHd4xncI541uwvYebCEY1K7k3vYkrclbkCAgcnxvHztFL/Jd3udLL4zx6RwplfsaVCPeN64/jjPdnSkg39cemSt9/zqpKH86qShfpOptUxOTWbxrScHWwyDwWd8uQvWK0aYOicjNis9zadihIaWU1rlv9R1gME97GKvpX6Zz18UljspqawmITqCLnGRDE625Nx1qJTCMidlVdaxpNiAF/c3GAx+xl0H8EIgFS+do6p/9mUeX5RVtrseoIfUORmht1inA+Fvy8puo7EzxCqT7y2wrKp+3eIQEQb1sOUs9VRRH+hWYAaDIex4DzgMfI9V0qlV+HIXXJM6J+OWrPS0x+wdWelprT6xoXn8HbMalBxPdKSDrIOljP7jYmKjHJ44jhXbsWIRduxGPHGemt8CREU4OHdiP248ZVibFt7aeCsrqLEAdx6qUVZDTHdZgyFcGaCqbVmvBfimrFKAM1LnZNQqRpiVnuZTMUJDy6h2KeVVLkQgNtI/yio2KoLLpwzixa+yKKuqblONuM25W+gSF8VVxw5us1z1lJXtBjxY4lnca5SVwRC2fCUi41V1fVsmabGyykpPuwQ8rr+xwHisYoRGWQUAW5HERUXgcLTderGZO3Mst581gupq9aw18qybqbMWyV5TU3c90vJtB3jg/R/555KfmDWpv6eie2vZ606g6OfO9uuVFENcVAT5pVX8sKsAqHFhGgyGsONE4BoR2YHlBhSsurk+LX1q9i7jrrjuWRbodv394P5pcEw4sm1/EUs35wHu9Ft36q23S8w7hdfhOS6eFGCPq0ysCuDDerW4g0o97OoVgUhj7tLGRIUjeify3pq9rNldwPtr93pq+LWWA0WWN7mXu0KFiDAoOZ4t+4r48ifrOxlviqAaDOHKz/wxSUvuhEtT52S8DbyXlZ7mKUaYOiejVcUIQ5Gi8irOe3KFpxafP4h0CO/+9gRPJXBfsRcE+yte5U9EhMunDGLN7gLeXbOnzcqqoarqg3rEe6paxEdHMLx36xW/wWAIHqq6U0QmYhWWAPifqq71dZ6WKKsZwLXAq6lzMoYABVgNGCOAT4B/ZqWnrfH1xKHEmt0FHkV13YlDapeUqeMqc3mVlKkp3WMdx71v1c58svPL+M//MnnsstYtCC/xNOILPWUFMGN8H/7w7ga+2XGI/JJKT5Hc1nDALlTrVatuXL+uLPlxHwAnHtGTyAjTtM9gCEfcXYJ/Dbzj3jVfROapqk+l+lrSIsRTjDB1ToanGGFWelqBbyKHLnY/oquPG8wfzxnT5vl2Hixh2t+/YOnm/TirXa260dZYVoGrZtAWusRGcfTg7qzMPMhX2w+SNqFvq+c65G6u6F1V/cKj+/PMl9updLq47sQhbZbXYDAEjeuAqapaAiAiD2M17vWvsvImKz2tzcUIQ5Ecd4B/QHf/rOUZ3COB1B7xZB0sZePeQiYO7ObzHCUV/l0QHAhOHN6TlZkHWb4tr03K6mCxbVnVKKsB3eP59PZpVDhdJhPQYAhvBKtViE01NZW+WkxI+FZE5DYR2SgiG0TkVRFpWV0dP7G/yF3OpwW151rKUYOsRoDr97SuIpXdzTUhgHXi2sqJR/QE4JvMQ82MbJyyympKK6uJjnCQWMeK7NctzigqgyH8eQH4RkTmisgDWOX6nvd1kqArKxHpD9wMTFbVcVixsMvaUwZP7blm+iX5whh3V9SNbhejr5T4uT1IIBjdtwvRkQ4yD5R4Wmb4ykHbBZgY7ZcFxgaDIbRQ1X8AvwQOun+uVtV/+jpPS1LXb2/qeFZ62j98PWkjcsSJSBUQj9Uvq92wq3q3pAVFSxnr7t/zY07rlFWp3dI+BLMBbaIjHYzp24U1uwtYn32YE4f39HmOhjIBDQZD+CMiy1X1RBEpwmpxJl7HVFV9ah3dEssqyf0zGfgN0N/9cwPQ5mwEVd0DPALswoqHHVbVT9o6ry8UuPs7tSWjrS7DeretDl9pGLgBAY50x+PWZhe06v018SpTZtJgCDYiMlBElorIJndo5pbWzqWqJ7p/J6lqF/dv+8cnRQUtUFZZ6WkPZKWnPYCVBXhUVnraHVnpaXcARwMDfD1hXUSkO3AeMAToBySIyJUNjJstIqtEZJXT6WzraT24XOpxuflTMfRKjCEhOoKC0iqPMvSFQC4K9icT3E311rVWWdlp68ayMhhCASdwh6qOBo4FfisibTJK3Nl/ze5rDl9iVoMA77tuJVbJ97ZyBrBDVfPcrY/fAY6vO0hV56nqZFWdHBnpvxu4p1hsdAQRfixrJCIMdlcPb01LjnCIWYEVtwL4aV9xq95/sNhywRo3oMEQfFQ1R1V/cL8uAjZhedLawpkN7PO5qoUvd/1XgG9T52QsxPI/XgC85OsJG2AXcKyIxANlwOnAKj/M2yKKK+zK5v63YFJ7xvNjTiE7D5Z43GUtpbTSSdHaT9jQZRec/Bu/y+YvhvZKIMIh7DxYQnlVNbE+LmI2MavavPTSSyxYsACAmTNn8rvf/S7IEhk6KyKSitXm/ptWvv83WL0Ph4rIOq9DScBXvs7XYssqKz3tQayMjnysKha/zEpP+6uvJ6yLqn4DvIVVa3C9W6Z5bZ23pdjrmRJj/a+sBnqqh/tuWRVXVFO89iPefeGx5gcHkZjICFJ7xONS2Lbfd+vKdgP2TDTKCuD4449nwIABrFq1imeffTbY4hg6HpF2OMX9M7uhQSKSCLwN3KqqrcsSg/8C5wKL3L/tn6NV9QqfBfdlcFZ6Wq0Ctv5CVe8H7m92YAAoKncrqwBYVv26Wi0vclvRRr60wklkl95o1X5/i+V3RqQksT2vhK37i3yuhVhjWZkEC4Dhw4fz/PPPU1xczMaNG4MtjqHj4VTVyU0NEJEoLEW1QFXfaWpsU6jqYaymiz935yYMxyrVh4igql/6Ml+L79CpczIEuAIYmpWe9ufUORmDgD5Z6Wnf+nLCUKPE3Y03EMrKToW313H5QmFpOaVbVhBaDegbZkRKEh9tyGVLbustK+MGtHj77bdJTEwkKiqKykrfE3MMhrYg1mLH54BN7vVR/pjzV8AtWAl5a7ASN1YCp/kyjy936KcAl/sEfwaKsLTvMb6cMNQorrAWswYiZtWni6WsclqhrAoOHfS8rqqqIiqqbW09AsnwFKsieqvcgMX16wJ2Zv7yl7+QmprKXXfdxSWXXBJscQydjxOAq4D1IrLGve9eVf2wDXPegqUnvlbVU0VkFPCAr5P4kg04NSs97bdAOUBWelo+EPZ3mGK3ZZUUAGXV121Z7WuFG/DwoQOe1/v3h7YrMNWd9bjrkO9ryjxuQBOzAqCyspLo6GhOOOEEzjvvvGCLY+hkqOpyVRVVnaCqR7p/2qKoAMpVtRxARGJUdTMw0tdJfFFWValzMiKwMgFJnZPRC8vSCmuKywNnWfVIjCHSIRwsqaTcxxbytrKadcllJCaGdi+nwT2sRJKdB0txuVreg7O8yqoLGBUhAXlYCEdsZZWZmcny5cuDLY7B4A+yRaQb8C6wRETeoxVVinxRVo8DC4GU1DkZDwLLgYd8PWGoYa9nCkQ2YIRDSHG7Ave7Szq1WK7DVnHY+/7wB7p2De0uuUmxUfRMjKbC6WJfUcutyANea6xMXUCLiooKoqOjeeqpp5gxY0awxTEY2oyqXqCqBao6F/gjVkzsfF/n8SV1fQHweywFlQOcn5We9qavJww1ApkNCDVJFjmHy1r8HlWltNBSVvn797F3b7uWSmwV9gLonT6k6dvuUTu2Z6ixrKKjo02ChSHsEYuB9raqLlPVRarq8x93i5WVOxtwMtAjKz3tSaAkdU7GFF9PGGp41lkFSlm5b8S+pK+XVVWTdNS5DPnNPM4443Tmz58fENn8yeBk2xXY8riVnXjS153ib4CVK1cyd+5coqOjqaqqQrXlblWDIdRQ6w/4XX/M5Ysb8CngOODn7u0i4N/+ECKYBLKCBeBxA/qSvl5c4UQiIunWZzBxcXHs27cvILL5k9aUlrI/E39Wuw93hg4dSp8+fYiOthJOjHVl6AB8LSJtzho32YCBtqy6WotdfbGsSiuqKVr9IcXrPiElJSXkswHBKi0FrbWsjLKyefTRR/nqq6+MsjJ0JE7FUljbRWSdiKyvU36pRfhyh+4w2YA//PADI0aMIDExkextmynZvJG4yCYXdbeaPm4Xly/p68UVTorXLcHRrTtDe/cOC8vKTl/fccBYVq1FVbnzzjuZO3cuV1xxBSNHjiQ21nw2hrDH56K1DdEpswEvvfRSli5dCsCmpW9z4L10tm0ITO3c1iwMLq2sprq0gPiuyWFjWQ3pZSur4hanr2cXWEknJmZlYVtR0dHRHHHEEZx77rkhvRjcYGghu4CTsDoE78QyeFJ8naS12YB7CeNswAMHDvDhh9Y6t+5jTwKgqqQoIOeyldU+X2JW5VW4Sg+T2K0Hd911F4888khAZPMnXWKj6JkYQ3mVi5wWWJGqyo48q+LFkJ4JgRYvLPBWVtnZ2bz//vuUlLSueafBEEL4Jd/Bl2zAWOBsrP5TpwEz3PvCjtLSUrp16wZAdVyy9buy5anlvtC7ixWz2l9U0WKL40DBYdRZSZfuPTjppJM444wzAiKbvxnqtq4y85ovu3SwpJLCcidJMZGm4robb2X1xRdfMHPmzLBYtmAwNMNUVfXkO6hqq/IdfHEDvgyMxXIHPgmMxupxFVZUVlZSWVnpqQpRhuVmcVYERlnFRkWQnBCN06UcKGnZwuB9+/JAHHRL7klubi4ZGRmUloZ+SdthbmXVkkaMmXmWxTC0V4JZEOzGW1mZBAtDB6JKRDz5DiLSqnwHXxIsRmalp0302l6aOidjra8nDDa2WyUhIYFql1JBNBIdR2QA75cpXWI5VFJJ7uFyeic1b4zGJPdh0F3vMvX4wSxbtozLLruMDRs2MHbs2MAJ6QcmDujGq9/u5oed+Vx34pAmx67fcxiAkX2S2kO0sKB3797k5OSQmJjIp59+ChhlZegQ2PkOvUXkQeAi4A++TuKLZbU6dU7GsfZG6pyMqcAKX08YbFSViy66iFGjRlFS6cQRE8/ou9/ht7+9MWDn7ON2BbZ0rVVhmRMRB90T4khJseKQ4ZARODnVcql+m3WoWZfnDzvzAThqUPeAyxUuRERE0KdPHxITE4mJsf5mjLIyhDuqauc7/BV39SNV9TnfwRfLairwi9Q5Gbvc24OATalzMtYDmpWeNsHXkweD5ORk3nzT+pzsEkiBqAvoja/p66tXLuPg4neIOu1v9O7d23pvGCirYb0S6Nc1lr2Hy7np1dX07RpLtSoul1KtSrULXC6lwllNxvocAI4b1iPIUocOubm5PP3001x22WXGDWjoULgrrW9uyxy+3KU7XFXNYnddwOz3H+fp7lv5zW9+E5Dz+Jq+vmPTWorXfkyPLv8mJaUbEPptQsDq/nntiUP4fxmbPMqoKU4a3tNT+cIAe/fu5c9//jNHHXUUJ598Mp9//jnjx48PtlgGQ6sQkSLccSpA6r5W1S6+zNesskqdk3EMsDsrPW2ne/sXwIXATmBuVnraIV9OGGy+/PJLzjvvPDIyMogbMBqAgp9WsXJlt8ApKx+rWBQcPIDEJNCjSyLdu3cnMjIyLCwrgOtOHELfrnHsOlRKhAMcIkQ4rB/PaxGiIoXTRvq81KJDY1tRMTExdO/enVNPPTXIEhkMrUdV/RqQboll9QxWujqpczJOBtKBm4AjgXlYwbI24e518iwwDkv7XquqK9s6b0MUFhZSUFBAdHS0p9RSVGxcQNez2PUBW+oGLCo4SER8V7rEReFwOFi8eDHDhg0LmHz+RERIm9A32GKEJd7ZgAUFBXz00UeccMIJDBo0KMiSGToLIvI8cA6wX1XH+WlOAa4AhqjqX9xV2Puq6re+zNOSBIsIL+vpUmBeVnra21npaX8EjvBJ6sZ5DFisqqOAicAmP81bj+JiK606ISHBU3E9Ji7Bsz8Q2BUaWuoGLDl8iIj4bnSJs54lTj/9dFJTUwMlXtDJzMzE5QrLyl1+xVtZ7d27l8svv5yvv/46yFIZOhkv4v+Qj70o+HL3djEBWhQckTonw7bATgc+9zrW5swEEekCnIzVkAtVrVTVgrbO2xi2UkpMTPT0soqNjw+osvK1ikW1QkSXnnSJtdaArVy5koULFwZMvmCyfft2hg0bxkMPhWXlLr9i1lkZgo2qfgn4O7Tjl0XBLVE2rwLLUudkHADKgP8BpM7JOAI47OsJG2AokAe8ICITge+BW1S1ll9ORGYDswHPP3Jr8FZWJbsKAOjeuy/dY6taPWdzdImLJDbKQUllNUXlVSTFNl3vrd/PH6Ksqpqucda4p59+mmXLlnHBBRcETMZgkZmZCTR/U3766afJzs7mT3/6kyetu6Mxffp0ioqKiI2N9VSuMMrK0AHwy6LgZi2rrPS0B4E7sMzDE7PS0+yMDgdW7KqtRAJHAU+r6iSgBJhTd5CqzlPVyao6OTKy9QbdqFGjuPrqq0lMTPTErH5+51/54IMPWj1nc4iIxxXYXNyq0umirKqaCIcQHx0B4Clm2xEb8e3ZsweAq6++uslxN954Iw899BArVwYklBkSREREkJiYSGRkpLGsDIEiUkRWef3Mbodz1l0UvBxrzZVPtOiun5WeVs9xnpWe9pOvJ2uEbCBbVb9xb79FA8rKX8yYMYMZMyyXbHFFNRC4XlbepHSJYceBEnIOl3NE78aTZHZk57L/zbn0O+kiRM623puSQnl5OUVFRXTp4lO2Z8iTlZUFWDHElrBjxw5OOeWUwAkURL777jtee+017r33Xo+yqqoKnMVv6JQ4VTUw/ZAaQVUXiMj3WGEkwVoU7HNegi8VLAKCquYCu0VkpHvX6cCPgTqfdyC/uMK6Eaz+dCFnn312oE4JeLW3byZu9VPWbsoyVxGnNePCaWGwr5SVWQuzTzjhhEbHeFsXO3bsCLhMwWL9+vX84x//oKSkhC5duvDdd99x6aWXBlssg6FNiMhLQK6q/ltVnwRy3VmHPhF0ZeXmJmCBu3vkkQSwT9Z1113nSQMvcVtWhXk5fPTRRwHNSEvp2rL09azdVqyiZ89eNe91l1wKh4XBvvLwww9z9913s3v37kY//+zsbM9r2xLriHgnWERGRjJ58mTPg4rB0B6IyKvASmCkiGSLyHV+mHaCd9KcO8Fikq+TBN7/1QJUdQ3QLqZpSUmJJ0BvZwN26WK55UpLSz3V2P1N3xZWsdi1NxeAPik1N6njjz+edevWccQR/lopEFoMHjyYqqoqcnJy6N+/f73jeXl5VoyxuLhDW1beygrg2WefZcKECUyZMiWYYhk6Ear68+ZH+YxDRLq7lRQikkwrdE9IKKv2pLi42KOQ7HVW3ZIS6x3zN31aaFntzbGUVf++NQtrk5KSOmTZHafTSVpaGoMHDwZg165dDSqrqVOnUlhYyFtvvUV1dXV7i9luVFRYLWRsZXXjjTdy5513GmVlCHceBb4SkbewMgIvAR70dZJQcQO2G94Kyc4G7NY1yXMsUNhVLJoruVThchDZvR99e9cu8Pr000+zdOnSgMkXDPbu3csnn3xCfHw8ADt37mx0rIhw8cUXc9lll7WXeO2O02n9PdrKKjo62mQDGsIeVX0Zq0TfPqxlSrNU1edeiJ1SWdmZZ7Zl1b9fXyZMmBDQ1HA7dT33cNMNGEecOov+s+fRu0vtvld//OMfeeONNwImXzCw40+nnHIKf/7znxvt1/XQQw9x++23U1ZWxpo1aygqKmpHKduPe+65B5fLRVSUtb7OKCtDR0FVf1TVJ1X1CVVtVQJdp3MDXn755SQnW32XitzK6mfTp/PLS84L6Hl7JcUQHeHgQHEFJRVOEhpJlz9QZCmzHom1F77aa606ErYlNWbMGM4///xGx3388ceoKt999x3Tpk3j448/5qyzzmonKdsX767J0dHRHtegwRCuuLMBb7GTLESkO/Coql7ryzydzrK68847ufZa6zOyLatA97MCiHAIQ3paFt32vMbdjR8++Qfyv3iRvl1rW1YpKSkdLnXdtqwGDRpEXl4eP/3U8NK9nTt3MnjwYIYMsboPd9Qki9dee42bb77Zs20sK0MHwS/ZgJ1OWR0+fBin00m1SymttIL1ubt3cuyxx7JkyZKAnvuIFCtWtnVf48oqb9s6qosO0L9bXK39vXv37nCWVWxsLMceeyyxsbH86le/4qKL6hfwdzqdZGdnk5qaSr9+/YiKiuqw6esrVqxgwYIFnu2lS5eSnp4eRIkMBr/gcFtTQOuzATuVslJVkpOTmTt3LiWVbqsqJhKHQ/jmm2889dgCxfDebmW1v2FlVVZZTVVxPpGJ3ejZgBuwo1lWd911l6d80uDBgxtMsMjOzqa6uprBgwcTERHBoEGDOqxlVVlZWavu5bBhwzxr7AyGMMbOBvyLiPwF+Ar4m6+TdCplVV5ejsvlstbsuNdYJcRE1GQHBjAbEGC4u8zS5tzCBo9v3XMArSqnW3JPHA6pdez+++/vsBYFWMrK7jXmTVFREaNHj/asMRsyZEinUVbz58/n7bffDqJEBkPbqZMNuB+rIPmxvs7TqRIsalVcr6ixrOzswEArq0mDugHw/c58ql1KRB2FtPony7Lo16f+07SdFNJRcLlcTJw4kVtvvZXrrrvO02Bw165ddOvWzTNu/Pjx/PhjTfLQfffd1yEL+kJ9ZfX444/To0cPLrzwwiBK1XYyMzPrPYQkJiYyYsQIz3ZOTg45OTm1xogIkyZZoY2srCwOHarpXBETE8OYMWNqJaQYQppoYBDWGqsdgM9PYZ1WWRV5Kau4uDhEJKDdggH6dYtjQPc4svPL2JRTyLj+XWsd/yn3MDEDxjJ21PB6792xYwfz5s3j17/+NUOHDg2onO1BTk4OGzZs8BRqtRcG79y5kwkTJjT6vo5axBYgKiqKrl1r/iY6QoJFdnY2xxxzTC1FAzBt2jS++OILwHLPT58+nfXr19caExMTQ3m5tS7xT3/6E6+8UntpzqJFizj33HMDJ7yhTYjICOAy4OfAQeB1QFT11NbM16mUla2MvLsEJ8ZG4nA4OPXUU+nXr1/AZTh5RC/++80uHvtsK5dPHYSzWql2uXC6lA+ylD5XPMx5P6ufKJOXl0d6ejonnHBCh1BWdnzKVlKjRo3i+eef58gjj6w17p577mHbtm28+eabAOTn57N8+XKOO+44evbs2a4yB5oXX3yx1nZHUFYDBgxg06ZN9Toe9+hRs+hdRPj222/55JNPao1xOGqiFLfeeqsnASc/P5+5c+eatP7QZzNW/8NzVXUbgIjc1trJOpWy6tGjB3PnzmX8+PHssGNW0dZH8Nlnn7WLDFdMHcR/v9nFkh/3seTH+gkT0ZEOTjqiV739dkHTjpIRaMffUlNTAejSpQu//OUv64379ttvPZXZAbZs2cLMmTN5//33Oeecc9pD1KARExMTcGu/PejduzczZ86sta+kpISkpCQeeOABbr/9dmJjY+uN8eaoo47iqKOO8mw31//MEBJciGVZLRWRxcBrWC1CWkWnSrDo27cv999/P6NGjfKUWmqPNVbejO3XlccuO5KTR/Ri2ohenDG6N9PHppA2vi+Dc5ZR/uotxEXWj8nYWWEdJSPQe42Vzfr16/nmm2/qjbMVGuBZaxXoZJPly5ezbNmyJsesWLGCw4drmmUvXLiwVnzthx9+4KOPPmrxOefOncsDDzzg2e4IltX333/PhRdeyNatW2vtj46Opri4mNLSUgoKCpgxYwYZGRlBktIQCFR1oapeCowCvgBuA1JE5GkR8XlVf6dSViUlJezduxen01mjrNyVJC655BJmz26Ppplw3pH9efnaKbx07RSevfoYnrlqMv++4iiOSq4if39OrSC7TVxcHElJSR1GWfXr14+ZM2fWarp4++23c8stt3i2q6ur2b17t8dVCNZTelxcXEAzAp1OJ9deey3vv/++Z1/dpA5VZeHChTzyyCOAlRgya9Ysj7Wnqpx11lmcffbZHDx4sEXn/fTTT/nyyy89288//zyffvppWy8nqOzevZt33nmnnoUYGRmJiFBRUUFJSQkff/yxp2t0c7hcLs4880xeeumlQIhs8DOqWqKqC1T1HGAAsIZWNNjtVMrq3XffpX///uzYsaNWNiBYAf/t27cHUzz279/fZP+i3r171wtUhyvXXHMN7733Xq19gwYNYteuXZ7tnJwcqqqqallWIkJqampAldWWLVvYunUrEydO9OybNGkSkyZN4oMPPvDIsWzZMpYvXw7gsbB27NhBeXk5Bw4c8CiputZiY1RWVnra14DltvaO7YQjdoJEbGztiiwiQkxMDBUVFZ7Yk/e1N4XD4WDp0qX1rDVD6KOqh1T1GVU9zdf3dipl1VA2oF2jLyEhIeCp683RnLLasGEDL7/8cjtKFDgaSj8fPHgwOTk5nptXRUUF06dPr1fgNtBrrTIzMwHo06cPBQUFlJWVsXbtWtasWcM777wDwIIFC9i3b5/nhjl+/Hjmz58PWArLu2FkaWlpi85bN3V90aJFHsstXGlMWQGtVlZgeRrsuQ2dg06rrGzLKskds7Kb+wWT5pRVQ//w4YjL5aJXr1787W+1F7Hb8avdu3cDVgWHxYsXc+KJJ9Ya9/DDD/Pf//43YPLZivCss87iySefrGXt2db3vffey+7du9mzZw9lZWWUlJR4OlBv376dSZMmeW7EDZWRaoi6yiojI6PDKKu4uLh6x6677jqOO+64Vimr2NjYWok3Bv8gIjNEZIuIbBMRn111gaRTZQPayig+Pr6mgkV06CirE088scm09LfffptPP/2Up59+uh2l8j/79+/n4MGDnj5WNt5NGJvqijxu3LiAypeZmUlCQgJdu3Zl+/btHuU1bNgwtm3bRlVVFdnZ2QwbNsxz/KabbqKwsJAlS5Zw9NFHAzQYe2yKnj170qtXTSZoTEyMZx1auBIbG8vgwYMbVFaPPvooYHkMRo0aRffu3euNaWpeY1n5FxGJAP4NnAlkA9+JyKLWtvTwN51OWcXFxREREUFxhVXE1s4GPProo6murqa0tLTBgrbjx49n6NChFBYWNtgEcdKkSQwaNIhDhw6xfPlypk2bVmuBpzfV1dV89tln9Z4M//Wvf3l6GTXEunXreOaZZ5g+fToiQmJiIqeffjoA//vf/+rFs7p168a0adNq7XM6nXz++eeUl5fjcDhITU1l3LhxqCqffPIJDocDEcHhcOBwOBg0aBBDhw7F6XTy7bff4nA4SE5OrlV9oC67du1i9erVnu2EhATOOOMMwMqgs2M43rEosNKTlyxZ4klRvv/++3nzzTdrZdgB5Obm8s477zBz5kwGDBjQqBxLly6lsLB2aatevXpx/PHHA7BkyZJ6Lrq+ffsye/ZsTj31VP7+97+zY8cOT+bhaaedxn/+8x+2bt2Ky+Vi5syZ5ObmEhERQVZWFlOmTPFc51NPPcWePXtITExk5cqVLFq0yHOODRs2NBgf9U6uAEvZlZaW8vnnn3PaaZaLv6HvuXv37px88smNXvPQoUNrdZr2nsM+Vl1d7YnHeTNixAhGjx5NRUUFixcvrnd89OjRjBgxgpKSkgaTQU4++eRGMzdVFZfLxbhx49i0aVODYxpj4sSJ9OvXj9zc3AZjgieccAI9e/YkOzubffv2eR4gAFavXu2pjPHdd9/VqwkaFxfnaUGzcuXKestFfPm/a+j76NmzJyeccALQ8N9gnz59mDp1KgAfffRRrYzQ4cOHM2bMmKY+mrYwBdimqpkAIvIacB4QEsoKVQ2JHyACWA180NzY+Ph4bQ3Lli3TJ554QlVVL33mKx189we6fGterTFZWVmK1Xq51s9jjz2mqqrr169v8Pjzzz+vqqorV65UQG+77bZG5XjvvfcanKOwsLBJ+V9++eVa40ePHu05duKJJ9ab75hjjqk3x/vvv19rzI033qiqqpWVlQ3KdPfdd6uq6sGDBz37RETXrl3bqJx1ZRk+fLjn2CmnnOLZv23btiav9xe/+IUOGjSo3v7vv/9eAX3nnXeafP/48ePrXc8ZZ5zhOT5kyJB6xy+44ALP8auuukoHDRqkb7zxhp5++un64osvKqBPP/20Avr555+rqqrT6dTIyEi955579LPPPtN33nlHp0+frkcffbTecccdGhsbq5s3b9b169ery+XS2267rcHPui7p6ekK6JgxYzz7jj/++Hrvmzp1quf4hAkT6h33/gy3b99e69jNN9+sqqplZWUNynTfffepqur+/fsbPP7Xv/61wXntnyeffLLR72fcuHE6a9asJr/D5li0aFGD5126dKmqqj7wwAMaGRmpZWVlqqqan5+v3bp18/ztXXzxxfXeO3DgQM/8M2bMqHfc+//uhBNOqHd8ypQpnuMNfR+nn36653hDf4Pnn3++53ivXr0a/H9sLUAFsMrrZ7bW3IMvAp712r4KeFL9eJ9vy08oWVa3AJuALoE6wcknn+x5Ai2xLas6TRD79u3LDz/8UO+9/fv3ByxXUEPH7XjL+PHjmTRpUoNjbCZMmMDcuXM588wza7lH6rrF6nLFFVcwadIkj2vIO4b13HPP1UsPjo+P57PPPiM7O9uziPLAgQMAvP/++/Tp08fjdoqIiGDFihWep13798CBAwFISkpi8eLFVFVVcemll/LYY4/x3HPP1ZNRVdmwYQOXXnopd999N1A7FjFv3jyKi4vp2rVrgy7PxYsX43A4OOuss8jNzaVPnz71xowYMYLIyEi++eYbLrjggkY/r9dff72eqygpKcnz+v3336+3jsnbGh4yZAjz58/nvPPO4+KLL6asrIxLLrnE08bDXvOVmZmJ0+lkyJAhPP7442zfvp3KykomTJjA0KFDKS8v5w9/+AOLFy/m8OHD3H777Vx11VWNym1zxx13MGPGjFruxBdeeKHB79nmtddeq3fN3gkqdtbiP/7xD0455RTP9x8dHd3g36y9vq9bt24NHrervvTv37/B401ZvlFRUVRWVrJ8+XLmzJnDc889x8iRIxsd3xAnn3xyg+e13cgulwun00lpaSmxsbGeYsnLli1j2LBhPPzww9xzzz315LJ54okn6nWm9v6/e/75533+Pnz5G/zss89wOp2e7aZi2i3EqaqTGznW0IJdbesJ/UawtaVbgw8APgNOI4CWlTen/H2pDr77A922v6jNc9Vl9uzZ2r17d3W5XH6f21euueaaWk+Kr7/+uo4ePVoPHDjQ6jmvv/56jYmJ0f379zd4/ODBg5qTk9OquY899lg97bTTVFV14sSJeu655zY47tRTT9Xx48c3eGz16tX61ltvaUVFRatksPnuu+/03//+t5aWltbaX1lZqZmZmep0OvXYY4/V7t27K6BLlizR2267TePi4jQ6Olrvuusu/fDDDxXQ7t2768SJE9skjz/4+uuvFdAPP/ww2KLoscceq2eeeaa+9dZbCjRprbeWZ555RgHNzs5WVdXNmzcroAsWLPD7ucIBoEQbvw8fB3zstX0PcE9j49v7J1SyAf8F/B5wNTZARGaLyCoRWeX9pNFa6i4K9icTJkwgPz+/0UWOGRkZtVKbA0lCQkKtJ79LLrmEH3/8sU3rd2699VYefvjhBoPmYFWIb8giagmDBg3yZAM2ZlkB/OxnP2P9+vUNfo7PPPMM11xzjf0P12omT57MjTfeyKhRo7j//vsBq+r7f//7X4YMGUJERAQ9e/b0WE5jx45lyJAhlJWVUVlZSWpqqsd6zM/Prxejaw8yMjK48sorPU/vU6dOxel0Mn369HaXpS6xsbG1Utd9TUhp6TmgJiuxqVR6A98Bw0VkiIhEY5VKWtTMe9qNoCsrETkH2K+q3zc1TlXnqepkVZ0cGdl2BWNnAwZKWfXs2bNBZVVcXMw555zDCy+84PfzNkRdZeUPRo0axS233OLpA+bN0qVL+dOf/tTitUV1SUlJ8QS0zzvvPI/bti5nn302ERERrFq1qtZ+VeWDDz7gzDPP9CkVuiFcLhfffPMNu3bt8ijmd955h2uuuYY33ngDsBIUIiMj+fOf/0zfvn1ruTaHDBnC4MGDPQkdttuwPdm8eTMLFiyo5YqKiIioVSQ2WNjrrGxF2tbvqyGMsmo5quoEfgd8jBWSeUNVNwZXqhqC/xcLJwAzRSQLq9DhaSIyP5AnrHYpZVXViEB8dITf5z/xxBPZv3+/J6PHm40bre++qTYY/iQxMZGKigqqq60Y3aOPPsqMGTPaPK/T6eS5556rVyn7o48+4m9/+1urbzy9e/fm8OHDVFRU8Mwzz3DllVc2OG7MmDHk5eVx/vnn19q/bt06srOz/Vbk9thjrR5xo0ePBmrS6+26gUOGDKGoqMiT3m4rpIULF/Kzn/2M2NhYT2sLe472xLZWbIWwfv16brjhhpBo5HnxxRdz+eWXt2qdVUs59thjefXVVz0x54SEBKZPn95qy7+jo6ofquoIVR2mqg8GWx5vgq6sVPUeVR2gqqlYZufnqtrwHcpPeFyA0ZEBad4mIo3Ou27dOqD9lJVde8+2rjZv3uyRoS04HA7S09OZO3durf1btmzhiCOOICKidQ8BdgB53759TbrxRKTBdTl2MdSzzz67Vef3xuFweJTVcccdB9QoL7uvlm013XfffYAV2P/6669rKdHU1FQyMjI488wz2yyTr9gKwFZWmZmZPPPMM+Tn57e7LHW57rrruPnmm+nRowfHHHNMswlGrWHQoEFcdtllnoae48aNY/HixbUquBvCg6Arq2BQUqfUUiB45JFHmDVrVr3969atIykpqVZx1kDyy1/+kp07d3pcdiUlJQ2673zF4XBw0003sXLlSr777jvP/p9++snnjC5vLrzwQjZv3syGDRuIiYlpsq5eZmYmp512mqeJH1hVvo855hi/PTm/9dZbrF692qNE77nnHpYsWeKpSjFlyhS++OILnn32WcCyZOpa1A6Hg7PPPjsobsC6lpX9OxDxIV8pLy/n8OHDXHTRRXz77be1OkT7C3tdpJ0FawhfQkpZqeoXalXmDSjt0R7k0KFDDaalrlu3jnHjxrVbzKB79+4MGjTIc77i4uJalc7bwjXXXENSUhKPP/44YLkGt2/f3uSC4ebo0aMHI0eO5ODBg1RVVZGcnNzo2N69e7NixYpai1nfeuutBhevtpb+/fvXaggZExPDGWecUctynjZtmt8+U3+TmJhISkoKLpeVuxTIZAZf+e1vf1uv7qO/2bx5M6eddprnoee9995j0KBBQS9abfCdkFJW7UVxO1hWEyZMwOl0snnz5lr7X3rppXYtl7Rt2zYefPBBzyp9f1lWUNMw8fXXXycnJ4e9e/cSFRXVJsuqpKSEf/3rX54+UPY6n4ZITExk2rRpfPjhh559ItKkgutsXHTRReTm5noSP0LJsoqJiaGgoIBf/OIXtSpM+JO6CRYFBQXs3r07JBJMDL4RSouC2w07EzApwMoKLEvKOz7V3unLmZmZ/OEPf+CUU06hX79+jBw5En9kU9rcdNNNfPXVV+zbt48jjzyS4uJi2rK0wOVycdttVudru4dXU5x99tncdttt7Nixg0ceeYT4+Hj+/ve/t/r8nYH4+PiAJDP4yrBhwygpKeGVV17xFAH2N3YWZ91swMaWXRhCl075eFETs/J/JqDN8OHDiY6OrpXMsG7dOh577LFa3WUDTd0Ei6eeesrjtvMHRxxxBN99953HVSYiTdY3bI7ExETP03BKSkqzCTB2IsWiRYuYP39+SCQOhBJr1qzhvPPO89Teu/baaykpKQmJbLg77riDgwcPcuDAgXr1H/2FSV3vOHRKZdUebsCoqCguuOCCWuVRPvroI2699dY2L1b1hbrKKlDk5+fzy1/+kttvv71N84gIvXv3RkS4/vrrmx0/fPhwLr/8cjIzMyksLPRbynpH4dChQyxatIi8vLxgi9IgycnJ9OjRI2BuSaOsOg6dWlkFYkGwN6+99hp33nmnZ3vdunUMGjQoIFlPjVFXWU2dOpX09HS/n+ess87ixRdfbHFX3Kbo3bs306dPZ86c5tvpiAgLFiwgKiqK6OhoT9Vzg0XdbMCFCxdy5ZVXtslVG05069aNRYsWeR5ihg8fzqxZs0IiZmfwjU6prOq2tA8k6i4IC/XjV+1BXWW1du3agLjKrrvuOgBPK/e20Lt3bzZu3OhZyNwS5s+fz6hRo/yWPNJRqKus1q5dy4IFC1q9Di7ciIqK4txzz/UsFZk1axZvv/22SbAIQzrlN2b3sgqkGxCsXjk9evTgyy+/pKKigs2bN9fqK9Qe9OnThwMHDnDttdfidDqpqKgIyA39qquuonv37p7FsW3h+eefZ/fu3fz5z39u0fjKykoiIiK49dZb23zujkZD66yioqICshg+VMnIyGDDhg3BFsPQRjplNmB7WVYDBw4kPz+fdevWkZKSQnV1dbtbVg6Hw1O01k7sCMSaoISEhHpN6FqLbYk2lbbuTXR0dKNFgzs7CQkJDB06tFYli87mArv00ku54YYbeOSRR7jlllv45JNPfG72aAg+nVpZBdqySklJoVevXqxbt46bb76Z4uLioDzR3n///Rx99NGetSyh7iqzF/mG6kLbcGL48OG1FsB2RmUVGxvrSaw4fPhwvQ7dhvCgU7oBizyWVWD99iLChAkTeOutt7jtttuIj48PyvqOJ554wtOy/uyzz26w6WEoYSdpmLiC/4mPj/c0TOwseCur8vJykwkYpnTKu0GNG7D164Fayu9+9zumTp0akCKdLcVuE9K3b18yMjJCPmPuoYceYvbs2Z76e4bWU1BQwOmnn867774LQHp6eqeL33grq7KyMqOswpRO7gYMfEbU+eefX6+NRXuTmJgY8HVW/qR3794888wzwRajQ6CqfP7558ycOTPYogSN2NhYj+vPWFbhS6dUVu21zipUsC2rTz/9lGuuuYaMjAwmTpwYbLEM7UDdbMCHHnqInJwcnnjiiWCK1a688MILnjjtWWedVa+4tCE86Bx36zq0RwWLUCIhIYHS0lLy8/PZs2dPp1ljY6ivrOzOx52JY445xvP6jjvuCKIk4Y2IXAzMBUYDU1R1VdPv8C+dNGbVPuusQoUlS5bw+eefU1xcDIR+NqDBf9hFi73XWXW2bMClS5d6mnK2Z6mzDsgGYBbwZTBO3umUlapSUtm53IDR0dGIiCduZZRV50FEmDx5sqdGZWdUVo8++ih/+tOfABg1ahRXXhnQRuQdFlXdpKpbgnX+znG39qK0shpViIuKIMLROVbxL1iwgLVr13r6PJn1S50L707OnVFZ1U1db0tXgA5ApIh4u+/mqeq8oEnjA51OWbXXguBQ4quvvuL1119n3rx5XHzxxSYbqhPTr1+/oC6jCAZmnVUtnKo6ubGDIvIp0FD/mPtU9b3AidU8Qb9ji8hA4GWsD8iFpekfC9T52mtBcChhZwPOmjWLWbNmBVscQztz9tlnc9xxx/HHP/6R119/PdjitDtxcXG1lJVpvNg4qhqyizCDrqwAJ3CHqv4gIknA9yKyRFUD0o2tM1pWCQkJlJeXU11dbTIBOyEbN26s1Vets2Esq45B0BMsVDVHVX9wvy4CNgH9A3W+zrbGCmpiVBdffLGnPqCh8xAdHe3JBrz88st56KGHgixR+/L73/+eZcuWoar87ne/48QTTwy2SGGJiFwgItnAcUCGiHzcnucPqTu2iKQCk4C2d/BrBDttvTMpq6SkJBITE9m7d6+normh8xATE+NRVitWrPBUYO8sDBw4kIEDBwJWZqChdajqQmBhsM4fdMvKRkQSgbeBW1W1sIHjs0VklYisakuX087oBrz++uspKioiISHBpK13Qrwtq86YDbhmzRqefPJJqqqqKCoq8qmppyF0CAllJSJRWIpqgaq+09AYVZ2nqpNVdbK90LE1FJVXAZAY23mUlU1xcbFJW++ETJ48mdGjRwNQUVHR6ZTV559/zk033URmZiZdunThueeeC7ZIhlYQ9Du2WA2engM2qeo/An2+w2WWsuoa13nWWmzYsIEHH3yQVatWccEFFwRbHEM7M29ezTKazmhZ2QkVBQUFtbYN4UUoWFYnAFcBp4nIGvfP2YE6ma2sunUiZZWfn89rr73Gsccey4wZM4ItjiGITJw4kcGDBwdbjHbFKKuOQdAtK1VdDrRbKYmC0s5nWdmuv7vuuivo7UoM7c/1119PXl4e77zzDitWrAi2OO2OrZzy8/NrbRvCi6Arq/bGY1nFdz5ldfjw4SBLYggG+/btIysrK9hiBA1vZRUTE2OUVZgSCm7AdqXAray6dELL6pprruH//b//F2RpDO2NnQ1YUlLCpEmTmD9/frBFalfOOussMjMzOeecc7jlllvo0aNHsEUytILOZ1mV2jGrzhNkTkxMJC4ujrKysk5XF85Qo6wqKipYs2YNhw4dCrZI7UpiYqJnycZdd91llFWY0uksq0Ol1nqT7gmdx7Lq1q0b27ZtA0x7kM6IrazstVadLRvQm549e2IlIBvCjU5lWVVVuzhQXIEI9EzsXKv47caLZp1V5+PII4+kurraKCtDWNOplFVeUQWq0CsphqiIzmVUnnfeeYCxrDojN998M4DHujbKyhCOdKo7ds5hq/Jyny6dLxto8+bNAIwcOTLIkhiCRVRUFKeccgp9+/YNtigGg890Kstq2/4iAIb26nyusBEjRjBp0iRGjRoVbFEM7czf/vY3Hn/8cbKzs1m6dGmwxTEYWkWnsawKy6t45stMAMb16xpkadqfiIgI9uzZY6qud0LKysrMd28IezqNskqKiWRcv64MSo5n1lEBa5cVsmzatInly5eza9euYItiaGfsGNWKFSsYPnw4K1euDLJEBoPvdBo3oIiQfuF4qpxK105UvaIuJhuw82Erq4MHD7Jt2zba0mLH0HkRkb8D5wKVwHbgl6pa0F7n7zSWFUB8dGSnVVTp6emAyQbsjNjKyl6+YLIBDa1kCTBOVScAPwH3tOfJO5Wy6swUFxcjIqYuWidk1KhRXH755Z6mg0ZZGVqDqn6iqrZZ/jUwoD3Pb5RVJ+Gll15CVc3q/U7ImWeeyYIFCzwV142yMviBa4GP2vOEnSZm1dnJyMhgzZo1wRbDEEQuvfRSunfvbtbadW4iRWSV1/Y8VfV05xSRT4E+DbzvPlV9zz3mPsAJLAiopHUQVW3P8/mFhIQELSkpCbYYBoPBEFaISKmqtjrLSkSuBm4ATlfVUv9J1jzGsjIYDAZDs4jIDOBuYFp7KyowlpXBYDB0GtpiWYnINiAGOOje9bWq3uA34ZrBWFYGg8FgaBZVPSKY5w9Ly0pEXEBZK98eiRUc7Gh0xOsy1xQ+dMTr6ojXFKeqYZkFHpbKqi2IyCpVnRxsOfxNR7wuc03hQ0e8ro54TeFMWGpYg8FgMHQujLIyGAwGQ8jTGZXVvOaHhCUd8brMNYUPHfG6OuI1hS2dLmZlMBgMhvCjM1pWBoPBYAgzOpWyEpEZIrJFRLaJyJxgy9NaRCRLRNaLyBq7zpeIJIvIEhHZ6v7dPdhyNoeIPC8i+0Vkg9e+Rq9DRO5xf3dbRGR6cKRumkauaa6I7HF/X2tE5GyvY+FwTQNFZKmIbBKRjSJyi3t/2H5XTVxTWH9XHRpV7RQ/QARWw7ChQDSwFhgTbLlaeS1ZQM86+/4GzHG/ngM8HGw5W3AdJwNHARuauw5gjPs7iwGGuL/LiGBfQwuvaS5wZwNjw+Wa+gJHuV8nYfUyGhPO31UT1xTW31VH/ulMltUUYJuqZqpqJfAacF6QZfIn5wEvuV+/BJwfPFFahqp+CRyqs7ux6zgPeE1VK1R1B7AN6zsNKRq5psYIl2vKUdUf3K+LgE1Af8L4u2rimhoj5K+po9OZlFV/YLfXdjZN/3GGMgp8IiLfi8hs974UVc0B6x8R6B006dpGY9cR7t/f70RkndtNaLvLwu6aRCQVmAR8Qwf5rupcE3SQ76qj0ZmUVUNdB8M1FfIEVT0K+BnwWxE5OdgCtQPh/P09DQwDjgRygEfd+8PqmkQkEXgbuFVVC5sa2sC+kLyuBq6pQ3xXHZHOpKyygYFe2wOAvUGSpU2o6l737/3AQix3xD4R6Qvg/r0/eBK2icauI2y/P1Xdp6rVquoC/kON+yhsrklEorBu6gtU9R337rD+rhq6po7wXXVUOpOy+g4YLiJDRCQauAxYFGSZfEZEEkQkyX4NnAVswLqWq93DrgbeC46Ebaax61gEXCYiMSIyBBgOfBsE+XzGvqG7uQDr+4IwuSYREeA5YJOq/sPrUNh+V41dU7h/Vx2ZTtMiRFWdIvI74GOszMDnVXVjkMVqDSnAQut/jUjgv6q6WES+A94QkeuAXcDFQZSxRYjIq8ApQE8RyQbuB9Jp4DpUdaOIvAH8iFUJ+7eqWh0UwZugkWs6RUSOxHIbZQHXQ/hcE3ACcBWwXkTWuPfdS3h/V41d08/D/LvqsJgKFgaDwWAIeTqTG9BgMBgMYYpRVgaDwWAIeYyyMhgMBkPIY5SVwWAwGEIeo6wMBoPBEPIYZWUwGAyGkMcoK0OnRkR6eLWDyPVqD1EsIk8F6Jy3isgvmjh+jog8EIhzGwzhillnZTC4EZG5QLGqPhLAc0QCP2C1p3A2MkbcY05Q1dJAyWIwhBPGsjIYGkBEThGRD9yv54rISyLyiViNL2eJyN/EaoC52F1jDhE5WkSWuavhf1yndI/NacAPtqISkZtF5Ed3le/XANR6gvwCOKddLtZgCAOMsjIYWsYwIA2rr9F8YKmqjgfKgDS3wnoCuEhVjwaeBx5sYJ4TgO+9tucAk1R1AnCD1/5VwEl+vwqDIUzpNLUBDYY28pGqVonIeqzakovd+9cDqcBIYBywxF23MQKrxURd+mI1+rNZBywQkXeBd7327wf6+U98gyG8McrKYGgZFQCq6hKRKq0J9rqw/o8E2KiqxzUzTxkQ67WdBpwMzAT+KCJj3S7CWPdYg8GAcQMaDP5iC9BLRI4Dq1eSiIxtYNwm4Aj3GAcwUFWXAr8HugGJ7nEjqGlPYTB0eoyyMhj8gKpWAhcBD4vIWmANcHwDQz/CsqTAchXOd7sWVwP/VNUC97FTgYxAymwwhBMmdd1gaGdEZCHwe1Xd2sjxFKw+Zae3r2QGQ+hilJXB0M6IyEggRVW/bOT4MUCVqq5pV8EMhhDGKCuDwWAwhDwmZmUwGAyGkMcoK4PBYDCEPEZZGQwGgyHkMcrKYDAYDCGPUVYGg8FgCHn+P+CNo7HhksTYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "t0 = np.arange(3, 288, 0.02)\n",
    "a0 = ramp(t0-3) - ramp(t0-4.5) - ramp(t0-8) + ramp(t0-9.5) \\\n",
    "    - 0.25*ramp(t0-27) + 0.25*ramp(t0-30) + 0.25*ramp(t0-32) - 0.25*ramp(t0-35) \\\n",
    "    + 0.5*ramp(t0-40) - 1.*ramp(t0-44) + 0.5*ramp(t0-48) \\\n",
    "    - 1*ramp(t0-60) + 2*ramp(t0 - 62) - 1*ramp(t0-64) \\\n",
    "    - 0.1*ramp(t0-79) + 0.4*ramp(t0-85) - 0.3*ramp(t0-87) \\\n",
    "    + 0.35*ramp(t0-95) - 0.7*ramp(t0-98) + 0.35*ramp(t0-101) \\\n",
    "    - 0.5*ramp(t0-101) + 1*ramp(t0-102.5) - 0.5*ramp(t0-104) \\\n",
    "    + 0.35*ramp(t0-104) - 0.7*ramp(t0-107) + 0.35*ramp(t0-110) \\\n",
    "    - 0.15*ramp(t0-110) + 0.3*ramp(t0-114) - 0.15*ramp(t0-118) \\\n",
    "    + jitter(0.25, np.pi / 2.0, t0, 132, 152) \\\n",
    "    + 2.*ramp(t0-160) - 2.*ramp(t0-161) - 2.*ramp(t0-163) + 2.*ramp(t0-164) \\\n",
    "    - 2.*ramp(t0 - 180) + 2*ramp(t0-181) + 2 *ramp(t0-183) - 2*ramp(t0-184) \\\n",
    "    + 2.0 * ramp(t0-210) - 2.0*ramp(t0-210.2) - 2.0*ramp(t0-216) + 2.0*ramp(t0-216.4)\\\n",
    "    + 2.0 * ramp(t0-218.4) - 2.0*ramp(t0-218.8)  - 2.0*ramp(t0 - 230) + 2.0*ramp(t0-230.2) \\\n",
    "    - 1.5*ramp(t0-240) + 1.5*ramp(t0-241) + 1.5*ramp(t0-243) - 1.5*ramp(t0-244)\n",
    "\n",
    "t0 = np.arange(0, 285, 0.02)\n",
    "v0 = cumtrapz(a0, t0, initial=0.) + 1.\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.set_xlabel('Time (s)')\n",
    "ax1.plot(t0, v0, color='tab:blue', linewidth=2.0, label='Speed')\n",
    "ax1.set_ylabel('Speed 'r'$(m/s)$', color='tab:blue')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(t0, a0, color='black', linestyle='--', linewidth=1.5, label='Acceleration')\n",
    "ax2.set_ylabel('Acceleration '+r'$(m/s^2)$', color='black')\n",
    "ax2.set_ylim(ax2.get_ylim()[0], 3 * ax2.get_ylim()[1])\n",
    "\n",
    "fig.legend()\n",
    "plt.title('Reference Trajectory')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAKE FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T17:57:06.227345Z",
     "start_time": "2020-05-13T17:57:06.211511Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_population(num, dim, rng):\n",
    "    \"\"\"\n",
    "    Generate flowers:\n",
    "        Input:\n",
    "            num: number of flowers (integer)\n",
    "            dim: number of parameters (integer)\n",
    "            rng: range number used in initialization (list or numpy array)\n",
    "        Output:\n",
    "            flws: initial position of the flowers (numpy array)\n",
    "    \"\"\"\n",
    "    flws = np.zeros((num,dim))\n",
    "    for i in range(dim):\n",
    "        lim = rng[i]\n",
    "        flws[:, i] = np.random.uniform(lim[0], lim[1], size=num)\n",
    "    return flws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def delayed_control_signal(i, u, u_list, td):\n",
    "    if i < td:\n",
    "        ut = 0.0\n",
    "    else:\n",
    "        if td == 0:\n",
    "            ut = u\n",
    "        else:\n",
    "            ut = u_list[i-td]\n",
    "    return ut\n",
    "_ = delayed_control_signal(1, 0.1, np.array([0.1, 0.2]), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def clip(a, a_min, a_max):\n",
    "    if a > a_max:\n",
    "        return a_max\n",
    "    elif a < a_min:\n",
    "        return a_min\n",
    "    else:\n",
    "        return a\n",
    "    \n",
    "_ = clip(2.0, -1.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steady state response parameters\n",
    "beta1, beta2, beta3 = param_ssr\n",
    "\n",
    "# System parameters\n",
    "a1, a2, a3, b1, b2, b3, b4, c1, c2, c3, c4, td11, td12, td13, td21, td22, td23 = param_dynamics\n",
    "td11 = int(np.around(td11))\n",
    "td12 = int(np.around(td12))\n",
    "td13 = int(np.around(td13))\n",
    "td21 = int(np.around(td21))\n",
    "td22 = int(np.around(td22))\n",
    "td23 = int(np.around(td23))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_min = -1.\n",
    "sat_max = 1.\n",
    "\n",
    "@njit\n",
    "def forward_propagation(t, v, param):    \n",
    "    kp, ki, kd = param\n",
    "\n",
    "    dt = np.mean(t[1:] - t[:-1])\n",
    "    \n",
    "    ki = ki * dt\n",
    "    kd = kd / dt    \n",
    "    \n",
    "    e_sum = 0.0\n",
    "    e_last = 0.0\n",
    "    e_int_state = 0 # 0 --> No Saturation || 1 --> Saturation (+) || -1 --> Saturation (-1)\n",
    "    is_start = True\n",
    "    \n",
    "    u1_list = np.empty(t.shape)\n",
    "    u2_list = np.empty(t.shape)\n",
    "    out = np.empty(t.shape)\n",
    "    y = 0.0\n",
    "    for i in range(t.shape[0]):     \n",
    "        # LONGITUDINAL CONTROLLER\n",
    "        sp = clip(v[i], 0.0, np.Inf)\n",
    "        sr = beta1 * (1 - np.exp(beta2*sp)) + beta3\n",
    "        sr = clip(sr, 0., sat_max) * 0.5\n",
    "        \n",
    "        err = sp - y\n",
    "        if e_int_state == 0:\n",
    "            e_sum += err\n",
    "        elif e_int_state == 1:\n",
    "            if err < 0:\n",
    "                e_sum += err\n",
    "        elif e_int_state == -1:\n",
    "            if err > 0:\n",
    "                e_sum += err\n",
    "        \n",
    "        if is_start:\n",
    "            temp = sr + kp * err + ki * e_sum + 0.\n",
    "            is_start = False\n",
    "        else:\n",
    "            temp = sr + kp * err + ki * e_sum + kd * (err - e_last)\n",
    "        \n",
    "        e_last = err\n",
    "\n",
    "        if temp > sat_max: # Saturation (+)\n",
    "            temp = sat_max\n",
    "            e_int_state = 1\n",
    "        elif temp < sat_min: # Saturation (-)\n",
    "            temp = sat_min\n",
    "            e_int_state = -1\n",
    "        else: # Not saturated\n",
    "            e_int_state = 0\n",
    "        \n",
    "        u1 = clip(temp, 0.0, sat_max)\n",
    "        u2 = clip(-temp, 0.0, -sat_min)\n",
    "        \n",
    "        # DYNAMICS     \n",
    "        u11t = delayed_control_signal(i, u1, u1_list, td11)\n",
    "        u12t = delayed_control_signal(i, u1, u1_list, td12)\n",
    "        u13t = delayed_control_signal(i, u1, u1_list, td13)\n",
    "        u21t = delayed_control_signal(i, u2, u2_list, td21)\n",
    "        u22t = delayed_control_signal(i, u2, u2_list, td22)\n",
    "        u23t = delayed_control_signal(i, u2, u2_list, td23)\n",
    "        \n",
    "        temp = 0.\n",
    "        if y != 0.:\n",
    "            temp = a1\n",
    "                    \n",
    "        y_dot = temp + a2 * y + a3 * y**2 \\\n",
    "                + b1 * u11t + b2 * np.exp(b3 * y + b4 * u12t) * u13t  \\\n",
    "                + c1 * u21t + c2 * np.exp(c3 * y + c4 * u22t) * u23t\n",
    "        \n",
    "        y += y_dot * dt\n",
    "        if y < 0.0:\n",
    "            y = 0.0\n",
    "\n",
    "        u1_list[i] = u1\n",
    "        u2_list[i] = u2\n",
    "        out[i] = y\n",
    "\n",
    "    return out, u1_list, u2_list\n",
    "_ = forward_propagation(np.arange(10, dtype=float), np.ones(10), np.array([0.1, 0.1, 0.1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378 µs ± 164 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit forward_propagation(t0, v0, np.array([0.2, 0.1550, 0.1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T17:57:06.287808Z",
     "start_time": "2020-05-13T17:57:06.273244Z"
    }
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def admissible(param):\n",
    "    kp, ki, kd = param\n",
    "    if kp < 0. or ki < 0. or kd < 0.:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "n_dim = 3\n",
    "_ = admissible(np.random.randn(n_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T17:57:06.308329Z",
     "start_time": "2020-05-13T17:57:06.291190Z"
    }
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def gradient(a, t):\n",
    "    dt = np.mean(t[1:]-t[:-1])    \n",
    "    out = np.zeros_like(a)\n",
    "    out[1:-1] = (a[2:] - a[:-2]) / 2 / dt\n",
    "    out[0] = out[1]\n",
    "    out[-1] = out[-2]\n",
    "    return out\n",
    "_ = gradient(v0, t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.array([[9.5, 27.], [35., 40.], [48., 60.], [64., 79.], [87., 95.], [118., 132.], [164., 180.], [184., 210.], [230.2, 240.], [244., t0[-1]+3.]]) -3\n",
    "direction = np.array([1, 0, 1, 0, 0, 1, 1, 0, 1, 0])\n",
    "\n",
    "@njit\n",
    "def max_os_sim(mv):\n",
    "    out = 0.\n",
    "    for i in range(mv.shape[0]):\n",
    "        for j in range(idx.shape[0]):\n",
    "            if idx[j,0] <= t0[i] and t0[i] <= idx[j,1]:\n",
    "                if direction[j] > 0.5:\n",
    "                    temp = mv[i] - v0[i]\n",
    "                else:\n",
    "                    temp = v0[i] - mv[i]\n",
    "                temp = temp / v0[i] * 100\n",
    "                temp = clip(temp, 0.0, np.Inf)\n",
    "                if temp > out:\n",
    "                    out = temp\n",
    "    return out\n",
    "_ = max_os_sim(np.zeros(v0.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def cost(t, v, param, lamda):\n",
    "    mv, cs1, cs2 = forward_propagation(t, v, param)\n",
    "    \n",
    "    error = v - mv\n",
    "    \n",
    "    mj = np.copy(cs1)\n",
    "    \n",
    "    max_os = max_os_sim(mv)\n",
    "    if max_os > lamda[1]: # max_os %\n",
    "        return np.Inf\n",
    "    loss = np.sum(error**2) + lamda[0] * np.sum(mj**2)\n",
    "    \n",
    "    M = t.shape[0]\n",
    "    return loss / M\n",
    "_ = cost(np.arange(10, dtype=float), np.ones(10), np.ones(3), np.array([0.001, 0.001]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T17:57:06.328315Z",
     "start_time": "2020-05-13T17:57:06.311336Z"
    }
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def mean_squared_error(t, v, param):\n",
    "    mv, _, _ = forward_propagation(t, v, param)\n",
    "    error = v - mv\n",
    "    cost = np.mean(error**2)\n",
    "    return cost\n",
    "_ = mean_squared_error(np.arange(10, dtype=float), np.ones(10), np.array([0.1, 0.1, 0.1]))\n",
    "\n",
    "@njit\n",
    "def mean_absolute_error(t, v, param):\n",
    "    mv, _, _ = forward_propagation(t, v, param)\n",
    "    error = v - mv\n",
    "    out = np.mean(np.abs(error))\n",
    "    return out\n",
    "_ = mean_absolute_error(np.arange(10, dtype=float), np.ones(10), np.array([0.1, 0.1, 0.1]))\n",
    "\n",
    "@njit\n",
    "def max_absolute_error(t, v, param):\n",
    "    mv, _, _ = forward_propagation(t, v, param)\n",
    "    error = v - mv\n",
    "    return np.max(np.abs(error))\n",
    "_ = max_absolute_error(np.arange(10, dtype=float), np.ones(10), np.array([0.1, 0.1, 0.1]))\n",
    "\n",
    "@njit\n",
    "def mean_absolute_jerk(t, v, param):\n",
    "    mv, _, _ = forward_propagation(t, v, param)\n",
    "    \n",
    "    ma = gradient(mv, t)\n",
    "    mj = gradient(ma, t)\n",
    "    return np.mean(np.abs(mj))\n",
    "_ = mean_absolute_jerk(np.arange(10, dtype=float), np.ones(10), np.array([0.1, 0.1, 0.1]))\n",
    "\n",
    "@njit\n",
    "def mean_squared_jerk(t, v, param):\n",
    "    mv, _, _ = forward_propagation(t, v, param)\n",
    "    \n",
    "    ma = gradient(mv, t)\n",
    "    mj = gradient(ma, t)\n",
    "    return np.mean(mj**2)\n",
    "_ = mean_squared_jerk(np.arange(10, dtype=float), np.ones(10), np.array([0.1, 0.1, 0.1]))\n",
    "\n",
    "@njit\n",
    "def max_percent_overshoot(t, v, param):\n",
    "    mv, _, _ = forward_propagation(t, v, param)\n",
    "    return max_os_sim(mv)\n",
    "_ = max_percent_overshoot(np.arange(10, dtype=float), np.ones(10), np.array([0.1, 0.1, 0.1]))\n",
    "\n",
    "@njit\n",
    "def mean_absolute_u_dot(t, v, param):\n",
    "    mv, cs1, cs2 = forward_propagation(t, v, param)\n",
    "    \n",
    "    cs1_dot = gradient(cs1, t)\n",
    "    cs2_dot = gradient(cs2, t)\n",
    "    return np.mean(np.abs(cs1_dot)+np.abs(cs2_dot))\n",
    "_ = mean_absolute_u_dot(np.arange(10, dtype=float), np.ones(10), np.array([0.1, 0.1, 0.1]))\n",
    "\n",
    "@njit\n",
    "def mean_squared_u_dot(t, v, param):\n",
    "    mv, cs1, cs2 = forward_propagation(t, v, param)\n",
    "    \n",
    "    cs1_dot = gradient(cs1, t)\n",
    "    cs2_dot = gradient(cs2, t)\n",
    "    return np.mean(np.abs(cs1_dot)**2+np.abs(cs2_dot)**2)\n",
    "_ = mean_squared_u_dot(np.arange(10, dtype=float), np.ones(10), np.array([0.1, 0.1, 0.1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T17:57:06.348846Z",
     "start_time": "2020-05-13T17:57:06.332107Z"
    }
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def calculate_total_cost(param, lamda):\n",
    "    if admissible(param):\n",
    "        return cost(t0, v0, param, lamda)\n",
    "    return np.Inf\n",
    "_ = calculate_total_cost(np.array([0.1, 0.1, 0.1]), np.array([0.001, 0.001]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T17:57:06.367975Z",
     "start_time": "2020-05-13T17:57:06.354989Z"
    }
   },
   "outputs": [],
   "source": [
    "@njit(parallel=True)\n",
    "def flowers_cost(flowers, lamda):\n",
    "    length = flowers.shape[0]\n",
    "    losses = np.zeros(length)\n",
    "    for ii in prange(length):\n",
    "        losses[ii] = calculate_total_cost(flowers[ii], lamda)\n",
    "    return losses\n",
    "_ = flowers_cost(np.array([[0.1, 0.1, 0.1], [0.1, 0.1, 0.1]]), np.array([0.001, 0.001]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Pollination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T16:38:55.858267Z",
     "start_time": "2020-05-05T16:38:55.830447Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@njit(parallel = True)\n",
    "def fpa(flowers, loss_flowers, global_, global_loss_, p, alpha, gamma, var, s0, lamda):\n",
    "    num = flowers.shape[0]\n",
    "    dim = flowers.shape[1]    \n",
    "    out = np.empty(flowers.shape)\n",
    "    temp = np.empty(dim)\n",
    "    loss = np.empty(loss_flowers.shape)\n",
    "    temp_loss = 0.\n",
    "    random_number = np.random.uniform(0., 1., num)\n",
    "    for i in prange(num):\n",
    "        # GLOBAL POLLINATION\n",
    "        if random_number[i] < p:\n",
    "            # Generate Levy Flight\n",
    "            upper = np.random.normal(0., np.sqrt(var), size=dim)\n",
    "            lower = np.abs(np.random.normal(0., 1., size=dim))**(1./alpha)\n",
    "            L = np.divide(upper, lower)\n",
    "            \n",
    "            for j in range(L.shape[0]):\n",
    "                if L[j] < s0:\n",
    "                    L[j] = s0\n",
    "            \n",
    "            temp = flowers[i] + gamma * L * (global_ - flowers[i])\n",
    "        # LOCAL POLLINATION\n",
    "        else:\n",
    "            while(True):\n",
    "                i1, i2 = np.random.randint(0, num, size=2)\n",
    "                if i1==i2 or i1==i or i2==i:\n",
    "                    if num <= 5: # For breaking the loop\n",
    "                        None\n",
    "                    else:\n",
    "                        continue\n",
    "                break\n",
    "            epsilon = np.random.uniform(0.,1.)\n",
    "            \n",
    "            temp = flowers[i] + epsilon * (flowers[i1] - flowers[i2])\n",
    "        \n",
    "        # CALCULATE COST\n",
    "        temp_loss = calculate_total_cost(temp, lamda)\n",
    "        if np.isnan(temp_loss):\n",
    "            temp_loss = np.Inf\n",
    "        \n",
    "        # UPDATE\n",
    "        if temp_loss < loss_flowers[i]:\n",
    "            out[i] = temp\n",
    "            loss[i] = temp_loss\n",
    "        else:\n",
    "            out[i] = flowers[i]\n",
    "            loss[i] = loss_flowers[i]\n",
    "    \n",
    "    min_idx = np.argmin(loss)\n",
    "    min_loss = loss[min_idx]\n",
    "    if global_loss_ > min_loss:\n",
    "        global_loss_new = min_loss\n",
    "        global_new = out[min_idx, :]\n",
    "    else:\n",
    "        global_new = global_\n",
    "        global_loss_new = global_loss_\n",
    "    \n",
    "    return out, loss, global_new, global_loss_new\n",
    "\n",
    "xx1 = np.ones((2, n_dim))\n",
    "xx2 = np.ones(2)\n",
    "xx3 = np.random.randn(n_dim)\n",
    "_ = fpa(xx1, xx2, xx3, 100.0, 0.8, 1.5, 0.1, 0.69, 0.1, np.array([0.001, 0.001]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMULATION (OPTIMIZATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T16:38:55.894842Z",
     "start_time": "2020-05-05T16:38:55.864001Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num = 50\n",
    "n_sim = 5\n",
    "n_itr = 5000\n",
    "\n",
    "r_kp = [0.0, 1.0]\n",
    "r_ki = [0.0, 1.0]\n",
    "r_kd = [0.0, 1.0]\n",
    "rng = [r_kp, r_ki, r_kd]\n",
    "dim = len(rng)\n",
    "\n",
    "s0 = 0.1\n",
    "p_threshold = 0.8\n",
    "alpha = 1.5\n",
    "gamma = 0.1\n",
    "var = (math.gamma(1+alpha)/alpha/math.gamma((1+alpha)/2) * np.sin(np.pi * alpha/2)/2**((alpha-1)/2))**(1/alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\lambda_0 = 1.0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T16:39:48.177174Z",
     "start_time": "2020-05-05T16:39:09.836931Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization: 1 ------------------------------------------\n",
      "Initializing ...\n",
      "Continue ...\n",
      "simulation: 1 || iteration: 1 || global_loss: 0.54597\n",
      "simulation: 1 || iteration: 501 || global_loss: 0.54575\n",
      "simulation: 1 || iteration: 1001 || global_loss: 0.54575\n",
      "simulation: 1 || iteration: 1501 || global_loss: 0.54575\n",
      "simulation: 1 || iteration: 2001 || global_loss: 0.54575\n",
      "simulation: 1 || iteration: 2501 || global_loss: 0.54575\n",
      "simulation: 1 || iteration: 3001 || global_loss: 0.54575\n",
      "simulation: 1 || iteration: 3501 || global_loss: 0.54575\n",
      "simulation: 1 || iteration: 4001 || global_loss: 0.54575\n",
      "simulation: 1 || iteration: 4501 || global_loss: 0.54575\n",
      "simulation: 0 || the best loss: 0.5457519560\n",
      "Optimization: 2 ------------------------------------------\n",
      "Initializing ...\n",
      "Continue ...\n",
      "simulation: 2 || iteration: 1 || global_loss: 0.54669\n",
      "simulation: 2 || iteration: 501 || global_loss: 0.54575\n",
      "simulation: 2 || iteration: 1001 || global_loss: 0.54575\n",
      "simulation: 2 || iteration: 1501 || global_loss: 0.54575\n",
      "simulation: 2 || iteration: 2001 || global_loss: 0.54575\n",
      "simulation: 2 || iteration: 2501 || global_loss: 0.54575\n",
      "simulation: 2 || iteration: 3001 || global_loss: 0.54575\n",
      "simulation: 2 || iteration: 3501 || global_loss: 0.54575\n",
      "simulation: 2 || iteration: 4001 || global_loss: 0.54575\n",
      "simulation: 2 || iteration: 4501 || global_loss: 0.54575\n",
      "simulation: 1 || the best loss: 0.5457519560\n",
      "Optimization: 3 ------------------------------------------\n",
      "Initializing ...\n",
      "Continue ...\n",
      "simulation: 3 || iteration: 1 || global_loss: 0.54966\n",
      "simulation: 3 || iteration: 501 || global_loss: 0.54575\n",
      "simulation: 3 || iteration: 1001 || global_loss: 0.54575\n",
      "simulation: 3 || iteration: 1501 || global_loss: 0.54575\n",
      "simulation: 3 || iteration: 2001 || global_loss: 0.54575\n",
      "simulation: 3 || iteration: 2501 || global_loss: 0.54575\n",
      "simulation: 3 || iteration: 3001 || global_loss: 0.54575\n",
      "simulation: 3 || iteration: 3501 || global_loss: 0.54575\n",
      "simulation: 3 || iteration: 4001 || global_loss: 0.54575\n",
      "simulation: 3 || iteration: 4501 || global_loss: 0.54575\n",
      "simulation: 2 || the best loss: 0.5457519560\n",
      "Optimization: 4 ------------------------------------------\n",
      "Initializing ...\n",
      "Continue ...\n",
      "simulation: 4 || iteration: 1 || global_loss: 0.54594\n",
      "simulation: 4 || iteration: 501 || global_loss: 0.54575\n",
      "simulation: 4 || iteration: 1001 || global_loss: 0.54575\n",
      "simulation: 4 || iteration: 1501 || global_loss: 0.54575\n",
      "simulation: 4 || iteration: 2001 || global_loss: 0.54575\n",
      "simulation: 4 || iteration: 2501 || global_loss: 0.54575\n",
      "simulation: 4 || iteration: 3001 || global_loss: 0.54575\n",
      "simulation: 4 || iteration: 3501 || global_loss: 0.54575\n",
      "simulation: 4 || iteration: 4001 || global_loss: 0.54575\n",
      "simulation: 4 || iteration: 4501 || global_loss: 0.54575\n",
      "simulation: 3 || the best loss: 0.5457519560\n",
      "Optimization: 5 ------------------------------------------\n",
      "Initializing ...\n",
      "Continue ...\n",
      "simulation: 5 || iteration: 1 || global_loss: 0.54597\n",
      "simulation: 5 || iteration: 501 || global_loss: 0.54575\n",
      "simulation: 5 || iteration: 1001 || global_loss: 0.54575\n",
      "simulation: 5 || iteration: 1501 || global_loss: 0.54575\n",
      "simulation: 5 || iteration: 2001 || global_loss: 0.54575\n",
      "simulation: 5 || iteration: 2501 || global_loss: 0.54575\n",
      "simulation: 5 || iteration: 3001 || global_loss: 0.54575\n",
      "simulation: 5 || iteration: 3501 || global_loss: 0.54575\n",
      "simulation: 5 || iteration: 4001 || global_loss: 0.54575\n",
      "simulation: 5 || iteration: 4501 || global_loss: 0.54575\n",
      "simulation: 4 || the best loss: 0.5457519560\n"
     ]
    }
   ],
   "source": [
    "lamda = np.array([1.0, np.Inf])\n",
    "\n",
    "param_history = np.zeros((n_sim, dim))\n",
    "loss_history = np.ones(n_sim) * np.Inf\n",
    "\n",
    "the_best_param_history = np.zeros((n_itr, dim))\n",
    "the_best_loss_history = np.zeros(n_itr)\n",
    "\n",
    "for j in range(n_sim):\n",
    "    print(f'Optimization: {j+1} ------------------------------------------')\n",
    "    \n",
    "    print('Initializing ...')\n",
    "    while True:\n",
    "        try:\n",
    "            flowers = generate_population(num, dim, rng)\n",
    "            global_ = None\n",
    "            global_loss_ = np.Inf\n",
    "\n",
    "            loss_flowers = flowers_cost(flowers, lamda)\n",
    "            loss_flowers[np.isnan(loss_flowers)] = np.Inf\n",
    "            min_idx = np.argmin(loss_flowers)\n",
    "            min_loss = loss_flowers[min_idx]\n",
    "            if global_loss_ > min_loss:\n",
    "                global_loss_ = min_loss\n",
    "                global_ = flowers[min_idx, :]\n",
    "\n",
    "            global_history = np.empty((n_itr, dim))\n",
    "            global_history[0] = global_\n",
    "            global_loss_history = np.empty(n_itr)\n",
    "            global_loss_history[0] = global_loss_\n",
    "            \n",
    "            # Biasanya di sini suka gagal, kalau inisialisasi flowers awal semuanya menyelisihi constraint\n",
    "            flowers, loss_flowers, global_, global_loss_ = fpa(flowers, loss_flowers, global_, global_loss_, p_threshold, alpha, gamma, var, s0, lamda)\n",
    "            break\n",
    "        except:\n",
    "            print('Re-Initializing ...')\n",
    "            \n",
    "    print('Continue ...')\n",
    "    for i in range(1, n_itr):\n",
    "        # Modified Flower Pollination Algorithm\n",
    "        flowers, loss_flowers, global_, global_loss_ = fpa(flowers, loss_flowers, global_, global_loss_, p_threshold, alpha, gamma, var, s0, lamda)\n",
    "\n",
    "        if (i-1) % 500 == 0:\n",
    "            print('simulation: {} || iteration: {} || global_loss: {:.5f}'.format(j+1, i, global_loss_))\n",
    "\n",
    "        global_history[i] = global_\n",
    "        global_loss_history[i] = global_loss_\n",
    "\n",
    "    if np.min(loss_history) > global_loss_history[-1]:\n",
    "        the_best_loss_history = np.copy(global_loss_history)\n",
    "        the_best_param_history = np.copy(global_history)\n",
    "        \n",
    "    param_history[j] = np.copy(global_history[-1])\n",
    "    loss_history[j] = np.copy(global_loss_history[-1])\n",
    "    \n",
    "    print('simulation: {} || the best loss: {:.10f}'.format(j, the_best_loss_history[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the simulation\n",
    "path_dir = 'lamda_0: '+str(int(lamda[0]))\n",
    "\n",
    "if os.path.exists(path_dir):\n",
    "    shutil.rmtree(path_dir)\n",
    "os.mkdir(path_dir)\n",
    "\n",
    "np.save(path_dir+'/param_history.npy', param_history)\n",
    "np.save(path_dir+'/loss_history.npy', loss_history)\n",
    "np.save(path_dir+'/the_best_loss_history.npy', the_best_loss_history)\n",
    "np.save(path_dir+'/the_best_param_history.npy', the_best_param_history)\n",
    "\n",
    "f = open(path_dir+\"/sim.cfg\", \"w+\")\n",
    "f.writelines('num: {} # The number of flowers\\n'.format(num))\n",
    "f.writelines('n_sim: {} # The number of simulation loop\\n'.format(n_sim))\n",
    "f.writelines('n_itr: {} # The number of iteration for each simulation\\n'.format(n_itr))\n",
    "f.writelines('\\n# Lambda value\\n')\n",
    "f.writelines('lambda0: {}'.format(lamda[0]))\n",
    "f.writelines('lambda1: {}'.format(lamda[1]))\n",
    "f.writelines('\\n# The boundary of the initialization value\\n')\n",
    "f.writelines('r_kp: {}\\n'.format(r_kp))\n",
    "f.writelines('r_ki: {}\\n'.format(r_ki))\n",
    "f.writelines('r_kd: {}\\n'.format(r_kd))\n",
    "f.writelines('\\n# The FPA hyperparameters\\n')\n",
    "f.writelines('s0: {}\\n'.format(s0))\n",
    "f.writelines('p_threshold: {}\\n'.format(p_threshold))\n",
    "f.writelines('alpha: {}\\n'.format(alpha))\n",
    "f.writelines('gamma: {}\\n'.format(gamma))\n",
    "f.writelines('var: {}\\n'.format(var))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda\n",
      "[ 1. inf]\n",
      "Parameters\n",
      "[0.86832365 1.3098594  0.03489674]\n",
      "Total loss: 0.545751955962023\n",
      "MAE: 0.008647641962054541\n",
      "MAJ: 0.4395976734433524\n",
      "MSJ: 2.788821799801277\n",
      "MAUD: 0.034541990012917224\n",
      "maximum %OS: 1.0488358856662832\n"
     ]
    }
   ],
   "source": [
    "print('Lambda')\n",
    "print(lamda)\n",
    "print('Parameters')\n",
    "print(global_)\n",
    "print('Total loss: {}'.format(global_loss_))\n",
    "print('MAE: {}'.format(mean_absolute_error(t0, v0, global_)))\n",
    "print('MAJ: {}'.format(mean_absolute_jerk(t0, v0, global_)))\n",
    "print('MSJ: {}'.format(mean_squared_jerk(t0, v0, global_)))\n",
    "print('MAUD: {}'.format(mean_absolute_u_dot(t0, v0, global_)))\n",
    "print('maximum %OS: {}'.format(max_percent_overshoot(t0, v0, global_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSUD: 27.819735851337157\n"
     ]
    }
   ],
   "source": [
    "print('MSUD: {}'.format(mean_squared_u_dot(t0, v0, np.array([0.56458294, 2.2533995,  0.07817718]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\lambda_0 = 5.0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T16:39:48.177174Z",
     "start_time": "2020-05-05T16:39:09.836931Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization: 1 ------------------------------------------\n",
      "Initializing ...\n",
      "Continue ...\n",
      "simulation: 1 || iteration: 1 || global_loss: 2.72290\n",
      "simulation: 1 || iteration: 501 || global_loss: 2.71661\n",
      "simulation: 1 || iteration: 1001 || global_loss: 2.71659\n",
      "simulation: 1 || iteration: 1501 || global_loss: 2.71659\n",
      "simulation: 1 || iteration: 2001 || global_loss: 2.71659\n",
      "simulation: 1 || iteration: 2501 || global_loss: 2.71659\n",
      "simulation: 1 || iteration: 3001 || global_loss: 2.71659\n",
      "simulation: 1 || iteration: 3501 || global_loss: 2.71659\n",
      "simulation: 1 || iteration: 4001 || global_loss: 2.71659\n",
      "simulation: 1 || iteration: 4501 || global_loss: 2.71659\n",
      "simulation: 0 || the best loss: 2.7165930803\n",
      "Optimization: 2 ------------------------------------------\n",
      "Initializing ...\n",
      "Continue ...\n",
      "simulation: 2 || iteration: 1 || global_loss: 2.72453\n",
      "simulation: 2 || iteration: 501 || global_loss: 2.71659\n",
      "simulation: 2 || iteration: 1001 || global_loss: 2.71659\n",
      "simulation: 2 || iteration: 1501 || global_loss: 2.71659\n",
      "simulation: 2 || iteration: 2001 || global_loss: 2.71659\n",
      "simulation: 2 || iteration: 2501 || global_loss: 2.71659\n",
      "simulation: 2 || iteration: 3001 || global_loss: 2.71659\n",
      "simulation: 2 || iteration: 3501 || global_loss: 2.71659\n",
      "simulation: 2 || iteration: 4001 || global_loss: 2.71659\n",
      "simulation: 2 || iteration: 4501 || global_loss: 2.71659\n",
      "simulation: 1 || the best loss: 2.7165930800\n",
      "Optimization: 3 ------------------------------------------\n",
      "Initializing ...\n",
      "Continue ...\n",
      "simulation: 3 || iteration: 1 || global_loss: 2.72426\n",
      "simulation: 3 || iteration: 501 || global_loss: 2.71659\n",
      "simulation: 3 || iteration: 1001 || global_loss: 2.71659\n",
      "simulation: 3 || iteration: 1501 || global_loss: 2.71659\n",
      "simulation: 3 || iteration: 2001 || global_loss: 2.71659\n",
      "simulation: 3 || iteration: 2501 || global_loss: 2.71659\n",
      "simulation: 3 || iteration: 3001 || global_loss: 2.71659\n",
      "simulation: 3 || iteration: 3501 || global_loss: 2.71659\n",
      "simulation: 3 || iteration: 4001 || global_loss: 2.71659\n",
      "simulation: 3 || iteration: 4501 || global_loss: 2.71659\n",
      "simulation: 2 || the best loss: 2.7165930800\n",
      "Optimization: 4 ------------------------------------------\n",
      "Initializing ...\n",
      "Continue ...\n",
      "simulation: 4 || iteration: 1 || global_loss: 2.72422\n",
      "simulation: 4 || iteration: 501 || global_loss: 2.71662\n",
      "simulation: 4 || iteration: 1001 || global_loss: 2.71659\n",
      "simulation: 4 || iteration: 1501 || global_loss: 2.71659\n",
      "simulation: 4 || iteration: 2001 || global_loss: 2.71659\n",
      "simulation: 4 || iteration: 2501 || global_loss: 2.71659\n",
      "simulation: 4 || iteration: 3001 || global_loss: 2.71659\n",
      "simulation: 4 || iteration: 3501 || global_loss: 2.71659\n",
      "simulation: 4 || iteration: 4001 || global_loss: 2.71659\n",
      "simulation: 4 || iteration: 4501 || global_loss: 2.71659\n",
      "simulation: 3 || the best loss: 2.7165930800\n",
      "Optimization: 5 ------------------------------------------\n",
      "Initializing ...\n",
      "Continue ...\n",
      "simulation: 5 || iteration: 1 || global_loss: 2.72392\n",
      "simulation: 5 || iteration: 501 || global_loss: 2.71661\n",
      "simulation: 5 || iteration: 1001 || global_loss: 2.71661\n",
      "simulation: 5 || iteration: 1501 || global_loss: 2.71661\n",
      "simulation: 5 || iteration: 2001 || global_loss: 2.71661\n",
      "simulation: 5 || iteration: 2501 || global_loss: 2.71661\n",
      "simulation: 5 || iteration: 3001 || global_loss: 2.71661\n",
      "simulation: 5 || iteration: 3501 || global_loss: 2.71661\n",
      "simulation: 5 || iteration: 4001 || global_loss: 2.71661\n",
      "simulation: 5 || iteration: 4501 || global_loss: 2.71661\n",
      "simulation: 4 || the best loss: 2.7165930800\n"
     ]
    }
   ],
   "source": [
    "lamda = np.array([5.0, np.Inf])\n",
    "\n",
    "param_history = np.zeros((n_sim, dim))\n",
    "loss_history = np.ones(n_sim) * np.Inf\n",
    "\n",
    "the_best_param_history = np.zeros((n_itr, dim))\n",
    "the_best_loss_history = np.zeros(n_itr)\n",
    "\n",
    "for j in range(n_sim):\n",
    "    print(f'Optimization: {j+1} ------------------------------------------')\n",
    "    \n",
    "    print('Initializing ...')\n",
    "    while True:\n",
    "        try:\n",
    "            flowers = generate_population(num, dim, rng)\n",
    "            global_ = None\n",
    "            global_loss_ = np.Inf\n",
    "\n",
    "            loss_flowers = flowers_cost(flowers, lamda)\n",
    "            loss_flowers[np.isnan(loss_flowers)] = np.Inf\n",
    "            min_idx = np.argmin(loss_flowers)\n",
    "            min_loss = loss_flowers[min_idx]\n",
    "            if global_loss_ > min_loss:\n",
    "                global_loss_ = min_loss\n",
    "                global_ = flowers[min_idx, :]\n",
    "\n",
    "            global_history = np.empty((n_itr, dim))\n",
    "            global_history[0] = global_\n",
    "            global_loss_history = np.empty(n_itr)\n",
    "            global_loss_history[0] = global_loss_\n",
    "            \n",
    "            # Biasanya di sini suka gagal, kalau inisialisasi flowers awal semuanya menyelisihi constraint\n",
    "            flowers, loss_flowers, global_, global_loss_ = fpa(flowers, loss_flowers, global_, global_loss_, p_threshold, alpha, gamma, var, s0, lamda)\n",
    "            break\n",
    "        except:\n",
    "            print('Re-Initializing ...')\n",
    "            \n",
    "    print('Continue ...')\n",
    "    for i in range(1, n_itr):\n",
    "        # Modified Flower Pollination Algorithm\n",
    "        flowers, loss_flowers, global_, global_loss_ = fpa(flowers, loss_flowers, global_, global_loss_, p_threshold, alpha, gamma, var, s0, lamda)\n",
    "\n",
    "        if (i-1) % 500 == 0:\n",
    "            print('simulation: {} || iteration: {} || global_loss: {:.5f}'.format(j+1, i, global_loss_))\n",
    "\n",
    "        global_history[i] = global_\n",
    "        global_loss_history[i] = global_loss_\n",
    "\n",
    "    if np.min(loss_history) > global_loss_history[-1]:\n",
    "        the_best_loss_history = np.copy(global_loss_history)\n",
    "        the_best_param_history = np.copy(global_history)\n",
    "        \n",
    "    param_history[j] = np.copy(global_history[-1])\n",
    "    loss_history[j] = np.copy(global_loss_history[-1])\n",
    "    \n",
    "    print('simulation: {} || the best loss: {:.10f}'.format(j, the_best_loss_history[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the simulation\n",
    "path_dir = 'lamda_0: '+str(int(lamda[0]))\n",
    "\n",
    "if os.path.exists(path_dir):\n",
    "    shutil.rmtree(path_dir)\n",
    "os.mkdir(path_dir)\n",
    "\n",
    "np.save(path_dir+'/param_history.npy', param_history)\n",
    "np.save(path_dir+'/loss_history.npy', loss_history)\n",
    "np.save(path_dir+'/the_best_loss_history.npy', the_best_loss_history)\n",
    "np.save(path_dir+'/the_best_param_history.npy', the_best_param_history)\n",
    "\n",
    "f = open(path_dir+\"/sim.cfg\", \"w+\")\n",
    "f.writelines('num: {} # The number of flowers\\n'.format(num))\n",
    "f.writelines('n_sim: {} # The number of simulation loop\\n'.format(n_sim))\n",
    "f.writelines('n_itr: {} # The number of iteration for each simulation\\n'.format(n_itr))\n",
    "f.writelines('\\n# Lambda value\\n')\n",
    "f.writelines('lambda0: {}'.format(lamda[0]))\n",
    "f.writelines('lambda1: {}'.format(lamda[1]))\n",
    "f.writelines('\\n# The boundary of the initialization value\\n')\n",
    "f.writelines('r_kp: {}\\n'.format(r_kp))\n",
    "f.writelines('r_ki: {}\\n'.format(r_ki))\n",
    "f.writelines('r_kd: {}\\n'.format(r_kd))\n",
    "f.writelines('\\n# The FPA hyperparameters\\n')\n",
    "f.writelines('s0: {}\\n'.format(s0))\n",
    "f.writelines('p_threshold: {}\\n'.format(p_threshold))\n",
    "f.writelines('alpha: {}\\n'.format(alpha))\n",
    "f.writelines('gamma: {}\\n'.format(gamma))\n",
    "f.writelines('var: {}\\n'.format(var))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda\n",
      "[ 5. inf]\n",
      "Parameters\n",
      "[1.13629804 0.01321665 0.05560901]\n",
      "Total loss: 2.716608552844447\n",
      "MAE: 0.12825106045878232\n",
      "MAJ: 32.21344889014294\n",
      "MSJ: 2425.9150909211567\n",
      "MAUD: 2.3775059489891968\n",
      "maximum %OS: 2.9243638131332395\n"
     ]
    }
   ],
   "source": [
    "print('Lambda')\n",
    "print(lamda)\n",
    "print('Parameters')\n",
    "print(global_)\n",
    "print('Total loss: {}'.format(global_loss_))\n",
    "print('MAE: {}'.format(mean_absolute_error(t0, v0, global_)))\n",
    "print('MAJ: {}'.format(mean_absolute_jerk(t0, v0, global_)))\n",
    "print('MSJ: {}'.format(mean_squared_jerk(t0, v0, global_)))\n",
    "print('MAUD: {}'.format(mean_absolute_u_dot(t0, v0, global_)))\n",
    "print('maximum %OS: {}'.format(max_percent_overshoot(t0, v0, global_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSUD: 27.819735851337157\n"
     ]
    }
   ],
   "source": [
    "print('MSUD: {}'.format(mean_squared_u_dot(t0, v0, np.array([0.56458294, 2.2533995,  0.07817718]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\lambda_0 = 10.0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T16:39:48.177174Z",
     "start_time": "2020-05-05T16:39:09.836931Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization: 1 ------------------------------------------\n",
      "Initializing ...\n",
      "Continue ...\n",
      "simulation: 1 || iteration: 1 || global_loss: 5.44599\n",
      "simulation: 1 || iteration: 501 || global_loss: 5.39675\n",
      "simulation: 1 || iteration: 1001 || global_loss: 5.39675\n",
      "simulation: 1 || iteration: 1501 || global_loss: 5.39675\n",
      "simulation: 1 || iteration: 2001 || global_loss: 5.39675\n",
      "simulation: 1 || iteration: 2501 || global_loss: 5.39675\n",
      "simulation: 1 || iteration: 3001 || global_loss: 5.39675\n",
      "simulation: 1 || iteration: 3501 || global_loss: 5.39675\n",
      "simulation: 1 || iteration: 4001 || global_loss: 5.39675\n",
      "simulation: 1 || iteration: 4501 || global_loss: 5.39675\n",
      "simulation: 0 || the best loss: 5.3967475999\n",
      "Optimization: 2 ------------------------------------------\n",
      "Initializing ...\n",
      "Continue ...\n",
      "simulation: 2 || iteration: 1 || global_loss: 5.43638\n",
      "simulation: 2 || iteration: 501 || global_loss: 5.39675\n",
      "simulation: 2 || iteration: 1001 || global_loss: 5.39675\n",
      "simulation: 2 || iteration: 1501 || global_loss: 5.39675\n",
      "simulation: 2 || iteration: 2001 || global_loss: 5.39675\n",
      "simulation: 2 || iteration: 2501 || global_loss: 5.39675\n",
      "simulation: 2 || iteration: 3001 || global_loss: 5.39675\n",
      "simulation: 2 || iteration: 3501 || global_loss: 5.39675\n",
      "simulation: 2 || iteration: 4001 || global_loss: 5.39675\n",
      "simulation: 2 || iteration: 4501 || global_loss: 5.39675\n",
      "simulation: 1 || the best loss: 5.3967475999\n",
      "Optimization: 3 ------------------------------------------\n",
      "Initializing ...\n",
      "Continue ...\n",
      "simulation: 3 || iteration: 1 || global_loss: 5.44547\n",
      "simulation: 3 || iteration: 501 || global_loss: 5.39675\n",
      "simulation: 3 || iteration: 1001 || global_loss: 5.39675\n",
      "simulation: 3 || iteration: 1501 || global_loss: 5.39675\n",
      "simulation: 3 || iteration: 2001 || global_loss: 5.39675\n",
      "simulation: 3 || iteration: 2501 || global_loss: 5.39675\n",
      "simulation: 3 || iteration: 3001 || global_loss: 5.39675\n",
      "simulation: 3 || iteration: 3501 || global_loss: 5.39675\n",
      "simulation: 3 || iteration: 4001 || global_loss: 5.39675\n",
      "simulation: 3 || iteration: 4501 || global_loss: 5.39675\n",
      "simulation: 2 || the best loss: 5.3967475999\n",
      "Optimization: 4 ------------------------------------------\n",
      "Initializing ...\n",
      "Continue ...\n",
      "simulation: 4 || iteration: 1 || global_loss: 5.44538\n",
      "simulation: 4 || iteration: 501 || global_loss: 5.39675\n",
      "simulation: 4 || iteration: 1001 || global_loss: 5.39675\n",
      "simulation: 4 || iteration: 1501 || global_loss: 5.39675\n",
      "simulation: 4 || iteration: 2001 || global_loss: 5.39675\n",
      "simulation: 4 || iteration: 2501 || global_loss: 5.39675\n",
      "simulation: 4 || iteration: 3001 || global_loss: 5.39675\n",
      "simulation: 4 || iteration: 3501 || global_loss: 5.39675\n",
      "simulation: 4 || iteration: 4001 || global_loss: 5.39675\n",
      "simulation: 4 || iteration: 4501 || global_loss: 5.39675\n",
      "simulation: 3 || the best loss: 5.3967475999\n",
      "Optimization: 5 ------------------------------------------\n",
      "Initializing ...\n",
      "Continue ...\n",
      "simulation: 5 || iteration: 1 || global_loss: 5.44088\n",
      "simulation: 5 || iteration: 501 || global_loss: 5.39675\n",
      "simulation: 5 || iteration: 1001 || global_loss: 5.39675\n",
      "simulation: 5 || iteration: 1501 || global_loss: 5.39675\n",
      "simulation: 5 || iteration: 2001 || global_loss: 5.39675\n",
      "simulation: 5 || iteration: 2501 || global_loss: 5.39675\n",
      "simulation: 5 || iteration: 3001 || global_loss: 5.39675\n",
      "simulation: 5 || iteration: 3501 || global_loss: 5.39675\n",
      "simulation: 5 || iteration: 4001 || global_loss: 5.39675\n",
      "simulation: 5 || iteration: 4501 || global_loss: 5.39675\n",
      "simulation: 4 || the best loss: 5.3967475999\n"
     ]
    }
   ],
   "source": [
    "lamda = np.array([10.0, np.Inf])\n",
    "\n",
    "param_history = np.zeros((n_sim, dim))\n",
    "loss_history = np.ones(n_sim) * np.Inf\n",
    "\n",
    "the_best_param_history = np.zeros((n_itr, dim))\n",
    "the_best_loss_history = np.zeros(n_itr)\n",
    "\n",
    "for j in range(n_sim):\n",
    "    print(f'Optimization: {j+1} ------------------------------------------')\n",
    "    \n",
    "    print('Initializing ...')\n",
    "    while True:\n",
    "        try:\n",
    "            flowers = generate_population(num, dim, rng)\n",
    "            global_ = None\n",
    "            global_loss_ = np.Inf\n",
    "\n",
    "            loss_flowers = flowers_cost(flowers, lamda)\n",
    "            loss_flowers[np.isnan(loss_flowers)] = np.Inf\n",
    "            min_idx = np.argmin(loss_flowers)\n",
    "            min_loss = loss_flowers[min_idx]\n",
    "            if global_loss_ > min_loss:\n",
    "                global_loss_ = min_loss\n",
    "                global_ = flowers[min_idx, :]\n",
    "\n",
    "            global_history = np.empty((n_itr, dim))\n",
    "            global_history[0] = global_\n",
    "            global_loss_history = np.empty(n_itr)\n",
    "            global_loss_history[0] = global_loss_\n",
    "            \n",
    "            # Biasanya di sini suka gagal, kalau inisialisasi flowers awal semuanya menyelisihi constraint\n",
    "            flowers, loss_flowers, global_, global_loss_ = fpa(flowers, loss_flowers, global_, global_loss_, p_threshold, alpha, gamma, var, s0, lamda)\n",
    "            break\n",
    "        except:\n",
    "            print('Re-Initializing ...')\n",
    "            \n",
    "    print('Continue ...')\n",
    "    for i in range(1, n_itr):\n",
    "        # Modified Flower Pollination Algorithm\n",
    "        flowers, loss_flowers, global_, global_loss_ = fpa(flowers, loss_flowers, global_, global_loss_, p_threshold, alpha, gamma, var, s0, lamda)\n",
    "\n",
    "        if (i-1) % 500 == 0:\n",
    "            print('simulation: {} || iteration: {} || global_loss: {:.5f}'.format(j+1, i, global_loss_))\n",
    "\n",
    "        global_history[i] = global_\n",
    "        global_loss_history[i] = global_loss_\n",
    "\n",
    "    if np.min(loss_history) > global_loss_history[-1]:\n",
    "        the_best_loss_history = np.copy(global_loss_history)\n",
    "        the_best_param_history = np.copy(global_history)\n",
    "        \n",
    "    param_history[j] = np.copy(global_history[-1])\n",
    "    loss_history[j] = np.copy(global_loss_history[-1])\n",
    "    \n",
    "    print('simulation: {} || the best loss: {:.10f}'.format(j, the_best_loss_history[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the simulation\n",
    "path_dir = 'lamda_0: '+str(int(lamda[0]))\n",
    "\n",
    "if os.path.exists(path_dir):\n",
    "    shutil.rmtree(path_dir)\n",
    "os.mkdir(path_dir)\n",
    "\n",
    "np.save(path_dir+'/param_history.npy', param_history)\n",
    "np.save(path_dir+'/loss_history.npy', loss_history)\n",
    "np.save(path_dir+'/the_best_loss_history.npy', the_best_loss_history)\n",
    "np.save(path_dir+'/the_best_param_history.npy', the_best_param_history)\n",
    "\n",
    "f = open(path_dir+\"/sim.cfg\", \"w+\")\n",
    "f.writelines('num: {} # The number of flowers\\n'.format(num))\n",
    "f.writelines('n_sim: {} # The number of simulation loop\\n'.format(n_sim))\n",
    "f.writelines('n_itr: {} # The number of iteration for each simulation\\n'.format(n_itr))\n",
    "f.writelines('\\n# Lambda value\\n')\n",
    "f.writelines('lambda0: {}'.format(lamda[0]))\n",
    "f.writelines('lambda1: {}'.format(lamda[1]))\n",
    "f.writelines('\\n# The boundary of the initialization value\\n')\n",
    "f.writelines('r_kp: {}\\n'.format(r_kp))\n",
    "f.writelines('r_ki: {}\\n'.format(r_ki))\n",
    "f.writelines('r_kd: {}\\n'.format(r_kd))\n",
    "f.writelines('\\n# The FPA hyperparameters\\n')\n",
    "f.writelines('s0: {}\\n'.format(s0))\n",
    "f.writelines('p_threshold: {}\\n'.format(p_threshold))\n",
    "f.writelines('alpha: {}\\n'.format(alpha))\n",
    "f.writelines('gamma: {}\\n'.format(gamma))\n",
    "f.writelines('var: {}\\n'.format(var))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda\n",
      "[10. inf]\n",
      "Parameters\n",
      "[1.14811251 0.00358267 0.0570368 ]\n",
      "Total loss: 5.396747599864064\n",
      "MAE: 0.2183290832307878\n",
      "MAJ: 35.37545682073073\n",
      "MSJ: 2692.4210396704752\n",
      "MAUD: 2.668098021779468\n",
      "maximum %OS: 4.096717326042249\n"
     ]
    }
   ],
   "source": [
    "print('Lambda')\n",
    "print(lamda)\n",
    "print('Parameters')\n",
    "print(global_)\n",
    "print('Total loss: {}'.format(global_loss_))\n",
    "print('MAE: {}'.format(mean_absolute_error(t0, v0, global_)))\n",
    "print('MAJ: {}'.format(mean_absolute_jerk(t0, v0, global_)))\n",
    "print('MSJ: {}'.format(mean_squared_jerk(t0, v0, global_)))\n",
    "print('MAUD: {}'.format(mean_absolute_u_dot(t0, v0, global_)))\n",
    "print('maximum %OS: {}'.format(max_percent_overshoot(t0, v0, global_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSUD: 27.819735851337157\n"
     ]
    }
   ],
   "source": [
    "print('MSUD: {}'.format(mean_squared_u_dot(t0, v0, np.array([0.56458294, 2.2533995,  0.07817718]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\lambda_0 = 20.0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T16:39:48.177174Z",
     "start_time": "2020-05-05T16:39:09.836931Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization: 1 ------------------------------------------\n",
      "Initializing ...\n",
      "Continue ...\n",
      "simulation: 1 || iteration: 1 || global_loss: 10.82841\n",
      "simulation: 1 || iteration: 501 || global_loss: 10.69878\n",
      "simulation: 1 || iteration: 1001 || global_loss: 10.69878\n",
      "simulation: 1 || iteration: 1501 || global_loss: 10.69878\n",
      "simulation: 1 || iteration: 2001 || global_loss: 10.69878\n",
      "simulation: 1 || iteration: 2501 || global_loss: 10.69878\n",
      "simulation: 1 || iteration: 3001 || global_loss: 10.69878\n",
      "simulation: 1 || iteration: 3501 || global_loss: 10.69878\n",
      "simulation: 1 || iteration: 4001 || global_loss: 10.69878\n",
      "simulation: 1 || iteration: 4501 || global_loss: 10.69878\n",
      "simulation: 0 || the best loss: 10.6987768814\n",
      "Optimization: 2 ------------------------------------------\n",
      "Initializing ...\n",
      "Continue ...\n",
      "simulation: 2 || iteration: 1 || global_loss: 10.87350\n",
      "simulation: 2 || iteration: 501 || global_loss: 10.69878\n",
      "simulation: 2 || iteration: 1001 || global_loss: 10.69878\n",
      "simulation: 2 || iteration: 1501 || global_loss: 10.69878\n",
      "simulation: 2 || iteration: 2001 || global_loss: 10.69878\n",
      "simulation: 2 || iteration: 2501 || global_loss: 10.69878\n",
      "simulation: 2 || iteration: 3001 || global_loss: 10.69878\n",
      "simulation: 2 || iteration: 3501 || global_loss: 10.69878\n",
      "simulation: 2 || iteration: 4001 || global_loss: 10.69878\n",
      "simulation: 2 || iteration: 4501 || global_loss: 10.69878\n",
      "simulation: 1 || the best loss: 10.6987768814\n",
      "Optimization: 3 ------------------------------------------\n",
      "Initializing ...\n",
      "Continue ...\n",
      "simulation: 3 || iteration: 1 || global_loss: 10.88585\n",
      "simulation: 3 || iteration: 501 || global_loss: 10.69878\n",
      "simulation: 3 || iteration: 1001 || global_loss: 10.69878\n",
      "simulation: 3 || iteration: 1501 || global_loss: 10.69878\n",
      "simulation: 3 || iteration: 2001 || global_loss: 10.69878\n",
      "simulation: 3 || iteration: 2501 || global_loss: 10.69878\n",
      "simulation: 3 || iteration: 3001 || global_loss: 10.69878\n",
      "simulation: 3 || iteration: 3501 || global_loss: 10.69878\n",
      "simulation: 3 || iteration: 4001 || global_loss: 10.69878\n",
      "simulation: 3 || iteration: 4501 || global_loss: 10.69878\n",
      "simulation: 2 || the best loss: 10.6987768814\n",
      "Optimization: 4 ------------------------------------------\n",
      "Initializing ...\n",
      "Continue ...\n",
      "simulation: 4 || iteration: 1 || global_loss: 10.88784\n",
      "simulation: 4 || iteration: 501 || global_loss: 10.69878\n",
      "simulation: 4 || iteration: 1001 || global_loss: 10.69878\n",
      "simulation: 4 || iteration: 1501 || global_loss: 10.69878\n",
      "simulation: 4 || iteration: 2001 || global_loss: 10.69878\n",
      "simulation: 4 || iteration: 2501 || global_loss: 10.69878\n",
      "simulation: 4 || iteration: 3001 || global_loss: 10.69878\n",
      "simulation: 4 || iteration: 3501 || global_loss: 10.69878\n",
      "simulation: 4 || iteration: 4001 || global_loss: 10.69878\n",
      "simulation: 4 || iteration: 4501 || global_loss: 10.69878\n",
      "simulation: 3 || the best loss: 10.6987768814\n",
      "Optimization: 5 ------------------------------------------\n",
      "Initializing ...\n",
      "Continue ...\n",
      "simulation: 5 || iteration: 1 || global_loss: 10.85655\n",
      "simulation: 5 || iteration: 501 || global_loss: 10.69878\n",
      "simulation: 5 || iteration: 1001 || global_loss: 10.69878\n",
      "simulation: 5 || iteration: 1501 || global_loss: 10.69878\n",
      "simulation: 5 || iteration: 2001 || global_loss: 10.69878\n",
      "simulation: 5 || iteration: 2501 || global_loss: 10.69878\n",
      "simulation: 5 || iteration: 3001 || global_loss: 10.69878\n",
      "simulation: 5 || iteration: 3501 || global_loss: 10.69878\n",
      "simulation: 5 || iteration: 4001 || global_loss: 10.69878\n",
      "simulation: 5 || iteration: 4501 || global_loss: 10.69878\n",
      "simulation: 4 || the best loss: 10.6987768814\n"
     ]
    }
   ],
   "source": [
    "lamda = np.array([20.0, np.Inf])\n",
    "\n",
    "param_history = np.zeros((n_sim, dim))\n",
    "loss_history = np.ones(n_sim) * np.Inf\n",
    "\n",
    "the_best_param_history = np.zeros((n_itr, dim))\n",
    "the_best_loss_history = np.zeros(n_itr)\n",
    "\n",
    "for j in range(n_sim):\n",
    "    print(f'Optimization: {j+1} ------------------------------------------')\n",
    "    \n",
    "    print('Initializing ...')\n",
    "    while True:\n",
    "        try:\n",
    "            flowers = generate_population(num, dim, rng)\n",
    "            global_ = None\n",
    "            global_loss_ = np.Inf\n",
    "\n",
    "            loss_flowers = flowers_cost(flowers, lamda)\n",
    "            loss_flowers[np.isnan(loss_flowers)] = np.Inf\n",
    "            min_idx = np.argmin(loss_flowers)\n",
    "            min_loss = loss_flowers[min_idx]\n",
    "            if global_loss_ > min_loss:\n",
    "                global_loss_ = min_loss\n",
    "                global_ = flowers[min_idx, :]\n",
    "\n",
    "            global_history = np.empty((n_itr, dim))\n",
    "            global_history[0] = global_\n",
    "            global_loss_history = np.empty(n_itr)\n",
    "            global_loss_history[0] = global_loss_\n",
    "            \n",
    "            # Biasanya di sini suka gagal, kalau inisialisasi flowers awal semuanya menyelisihi constraint\n",
    "            flowers, loss_flowers, global_, global_loss_ = fpa(flowers, loss_flowers, global_, global_loss_, p_threshold, alpha, gamma, var, s0, lamda)\n",
    "            break\n",
    "        except:\n",
    "            print('Re-Initializing ...')\n",
    "            \n",
    "    print('Continue ...')\n",
    "    for i in range(1, n_itr):\n",
    "        # Modified Flower Pollination Algorithm\n",
    "        flowers, loss_flowers, global_, global_loss_ = fpa(flowers, loss_flowers, global_, global_loss_, p_threshold, alpha, gamma, var, s0, lamda)\n",
    "\n",
    "        if (i-1) % 500 == 0:\n",
    "            print('simulation: {} || iteration: {} || global_loss: {:.5f}'.format(j+1, i, global_loss_))\n",
    "\n",
    "        global_history[i] = global_\n",
    "        global_loss_history[i] = global_loss_\n",
    "\n",
    "    if np.min(loss_history) > global_loss_history[-1]:\n",
    "        the_best_loss_history = np.copy(global_loss_history)\n",
    "        the_best_param_history = np.copy(global_history)\n",
    "        \n",
    "    param_history[j] = np.copy(global_history[-1])\n",
    "    loss_history[j] = np.copy(global_loss_history[-1])\n",
    "    \n",
    "    print('simulation: {} || the best loss: {:.10f}'.format(j, the_best_loss_history[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the simulation\n",
    "path_dir = 'lamda_0: '+str(int(lamda[0]))\n",
    "\n",
    "if os.path.exists(path_dir):\n",
    "    shutil.rmtree(path_dir)\n",
    "os.mkdir(path_dir)\n",
    "\n",
    "np.save(path_dir+'/param_history.npy', param_history)\n",
    "np.save(path_dir+'/loss_history.npy', loss_history)\n",
    "np.save(path_dir+'/the_best_loss_history.npy', the_best_loss_history)\n",
    "np.save(path_dir+'/the_best_param_history.npy', the_best_param_history)\n",
    "\n",
    "f = open(path_dir+\"/sim.cfg\", \"w+\")\n",
    "f.writelines('num: {} # The number of flowers\\n'.format(num))\n",
    "f.writelines('n_sim: {} # The number of simulation loop\\n'.format(n_sim))\n",
    "f.writelines('n_itr: {} # The number of iteration for each simulation\\n'.format(n_itr))\n",
    "f.writelines('\\n# Lambda value\\n')\n",
    "f.writelines('lambda0: {}'.format(lamda[0]))\n",
    "f.writelines('lambda1: {}'.format(lamda[1]))\n",
    "f.writelines('\\n# The boundary of the initialization value\\n')\n",
    "f.writelines('r_kp: {}\\n'.format(r_kp))\n",
    "f.writelines('r_ki: {}\\n'.format(r_ki))\n",
    "f.writelines('r_kd: {}\\n'.format(r_kd))\n",
    "f.writelines('\\n# The FPA hyperparameters\\n')\n",
    "f.writelines('s0: {}\\n'.format(s0))\n",
    "f.writelines('p_threshold: {}\\n'.format(p_threshold))\n",
    "f.writelines('alpha: {}\\n'.format(alpha))\n",
    "f.writelines('gamma: {}\\n'.format(gamma))\n",
    "f.writelines('var: {}\\n'.format(var))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda\n",
      "[20. inf]\n",
      "Parameters\n",
      "[8.21123523e-01 2.82292994e-04 6.04652868e-02]\n",
      "Total loss: 10.698776881411836\n",
      "MAE: 0.40839855831936683\n",
      "MAJ: 21.698083964492426\n",
      "MSJ: 1726.3633343689864\n",
      "MAUD: 1.5409244077744297\n",
      "maximum %OS: 6.23311088780539\n"
     ]
    }
   ],
   "source": [
    "print('Lambda')\n",
    "print(lamda)\n",
    "print('Parameters')\n",
    "print(global_)\n",
    "print('Total loss: {}'.format(global_loss_))\n",
    "print('MAE: {}'.format(mean_absolute_error(t0, v0, global_)))\n",
    "print('MAJ: {}'.format(mean_absolute_jerk(t0, v0, global_)))\n",
    "print('MSJ: {}'.format(mean_squared_jerk(t0, v0, global_)))\n",
    "print('MAUD: {}'.format(mean_absolute_u_dot(t0, v0, global_)))\n",
    "print('maximum %OS: {}'.format(max_percent_overshoot(t0, v0, global_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSUD: 27.819735851337157\n"
     ]
    }
   ],
   "source": [
    "print('MSUD: {}'.format(mean_squared_u_dot(t0, v0, np.array([0.56458294, 2.2533995,  0.07817718]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\lambda_0 = 30.0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T16:39:48.177174Z",
     "start_time": "2020-05-05T16:39:09.836931Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization: 1 ------------------------------------------\n",
      "Initializing ...\n",
      "Continue ...\n",
      "simulation: 1 || iteration: 1 || global_loss: 16.30132\n",
      "simulation: 1 || iteration: 501 || global_loss: 15.91963\n",
      "simulation: 1 || iteration: 1001 || global_loss: 15.91963\n",
      "simulation: 1 || iteration: 1501 || global_loss: 15.91963\n",
      "simulation: 1 || iteration: 2001 || global_loss: 15.91963\n",
      "simulation: 1 || iteration: 2501 || global_loss: 15.91963\n",
      "simulation: 1 || iteration: 3001 || global_loss: 15.91963\n",
      "simulation: 1 || iteration: 3501 || global_loss: 15.91963\n",
      "simulation: 1 || iteration: 4001 || global_loss: 15.91963\n",
      "simulation: 1 || iteration: 4501 || global_loss: 15.91963\n",
      "simulation: 0 || the best loss: 15.9196255601\n",
      "Optimization: 2 ------------------------------------------\n",
      "Initializing ...\n",
      "Continue ...\n",
      "simulation: 2 || iteration: 1 || global_loss: 16.30542\n",
      "simulation: 2 || iteration: 501 || global_loss: 15.91963\n",
      "simulation: 2 || iteration: 1001 || global_loss: 15.91963\n",
      "simulation: 2 || iteration: 1501 || global_loss: 15.91963\n",
      "simulation: 2 || iteration: 2001 || global_loss: 15.91963\n",
      "simulation: 2 || iteration: 2501 || global_loss: 15.91963\n",
      "simulation: 2 || iteration: 3001 || global_loss: 15.91963\n",
      "simulation: 2 || iteration: 3501 || global_loss: 15.91963\n",
      "simulation: 2 || iteration: 4001 || global_loss: 15.91963\n",
      "simulation: 2 || iteration: 4501 || global_loss: 15.91963\n",
      "simulation: 1 || the best loss: 15.9196255601\n",
      "Optimization: 3 ------------------------------------------\n",
      "Initializing ...\n",
      "Continue ...\n",
      "simulation: 3 || iteration: 1 || global_loss: 16.32244\n",
      "simulation: 3 || iteration: 501 || global_loss: 15.91963\n",
      "simulation: 3 || iteration: 1001 || global_loss: 15.91963\n",
      "simulation: 3 || iteration: 1501 || global_loss: 15.91963\n",
      "simulation: 3 || iteration: 2001 || global_loss: 15.91963\n",
      "simulation: 3 || iteration: 2501 || global_loss: 15.91963\n",
      "simulation: 3 || iteration: 3001 || global_loss: 15.91963\n",
      "simulation: 3 || iteration: 3501 || global_loss: 15.91963\n",
      "simulation: 3 || iteration: 4001 || global_loss: 15.91963\n",
      "simulation: 3 || iteration: 4501 || global_loss: 15.91963\n",
      "simulation: 2 || the best loss: 15.9196255593\n",
      "Optimization: 4 ------------------------------------------\n",
      "Initializing ...\n",
      "Continue ...\n",
      "simulation: 4 || iteration: 1 || global_loss: 16.23909\n",
      "simulation: 4 || iteration: 501 || global_loss: 15.91963\n",
      "simulation: 4 || iteration: 1001 || global_loss: 15.91963\n",
      "simulation: 4 || iteration: 1501 || global_loss: 15.91963\n",
      "simulation: 4 || iteration: 2001 || global_loss: 15.91963\n",
      "simulation: 4 || iteration: 2501 || global_loss: 15.91963\n",
      "simulation: 4 || iteration: 3001 || global_loss: 15.91963\n",
      "simulation: 4 || iteration: 3501 || global_loss: 15.91963\n",
      "simulation: 4 || iteration: 4001 || global_loss: 15.91963\n",
      "simulation: 4 || iteration: 4501 || global_loss: 15.91963\n",
      "simulation: 3 || the best loss: 15.9196255593\n",
      "Optimization: 5 ------------------------------------------\n",
      "Initializing ...\n",
      "Continue ...\n",
      "simulation: 5 || iteration: 1 || global_loss: 16.30763\n",
      "simulation: 5 || iteration: 501 || global_loss: 15.91963\n",
      "simulation: 5 || iteration: 1001 || global_loss: 15.91963\n",
      "simulation: 5 || iteration: 1501 || global_loss: 15.91963\n",
      "simulation: 5 || iteration: 2001 || global_loss: 15.91963\n",
      "simulation: 5 || iteration: 2501 || global_loss: 15.91963\n",
      "simulation: 5 || iteration: 3001 || global_loss: 15.91963\n",
      "simulation: 5 || iteration: 3501 || global_loss: 15.91963\n",
      "simulation: 5 || iteration: 4001 || global_loss: 15.91963\n",
      "simulation: 5 || iteration: 4501 || global_loss: 15.91963\n",
      "simulation: 4 || the best loss: 15.9196255593\n"
     ]
    }
   ],
   "source": [
    "lamda = np.array([30.0, np.Inf])\n",
    "\n",
    "param_history = np.zeros((n_sim, dim))\n",
    "loss_history = np.ones(n_sim) * np.Inf\n",
    "\n",
    "the_best_param_history = np.zeros((n_itr, dim))\n",
    "the_best_loss_history = np.zeros(n_itr)\n",
    "\n",
    "for j in range(n_sim):\n",
    "    print(f'Optimization: {j+1} ------------------------------------------')\n",
    "    \n",
    "    print('Initializing ...')\n",
    "    while True:\n",
    "        try:\n",
    "            flowers = generate_population(num, dim, rng)\n",
    "            global_ = None\n",
    "            global_loss_ = np.Inf\n",
    "\n",
    "            loss_flowers = flowers_cost(flowers, lamda)\n",
    "            loss_flowers[np.isnan(loss_flowers)] = np.Inf\n",
    "            min_idx = np.argmin(loss_flowers)\n",
    "            min_loss = loss_flowers[min_idx]\n",
    "            if global_loss_ > min_loss:\n",
    "                global_loss_ = min_loss\n",
    "                global_ = flowers[min_idx, :]\n",
    "\n",
    "            global_history = np.empty((n_itr, dim))\n",
    "            global_history[0] = global_\n",
    "            global_loss_history = np.empty(n_itr)\n",
    "            global_loss_history[0] = global_loss_\n",
    "            \n",
    "            # Biasanya di sini suka gagal, kalau inisialisasi flowers awal semuanya menyelisihi constraint\n",
    "            flowers, loss_flowers, global_, global_loss_ = fpa(flowers, loss_flowers, global_, global_loss_, p_threshold, alpha, gamma, var, s0, lamda)\n",
    "            break\n",
    "        except:\n",
    "            print('Re-Initializing ...')\n",
    "            \n",
    "    print('Continue ...')\n",
    "    for i in range(1, n_itr):\n",
    "        # Modified Flower Pollination Algorithm\n",
    "        flowers, loss_flowers, global_, global_loss_ = fpa(flowers, loss_flowers, global_, global_loss_, p_threshold, alpha, gamma, var, s0, lamda)\n",
    "\n",
    "        if (i-1) % 500 == 0:\n",
    "            print('simulation: {} || iteration: {} || global_loss: {:.5f}'.format(j+1, i, global_loss_))\n",
    "\n",
    "        global_history[i] = global_\n",
    "        global_loss_history[i] = global_loss_\n",
    "\n",
    "    if np.min(loss_history) > global_loss_history[-1]:\n",
    "        the_best_loss_history = np.copy(global_loss_history)\n",
    "        the_best_param_history = np.copy(global_history)\n",
    "        \n",
    "    param_history[j] = np.copy(global_history[-1])\n",
    "    loss_history[j] = np.copy(global_loss_history[-1])\n",
    "    \n",
    "    print('simulation: {} || the best loss: {:.10f}'.format(j, the_best_loss_history[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the simulation\n",
    "path_dir = 'lamda_0: '+str(int(lamda[0]))\n",
    "\n",
    "if os.path.exists(path_dir):\n",
    "    shutil.rmtree(path_dir)\n",
    "os.mkdir(path_dir)\n",
    "\n",
    "np.save(path_dir+'/param_history.npy', param_history)\n",
    "np.save(path_dir+'/loss_history.npy', loss_history)\n",
    "np.save(path_dir+'/the_best_loss_history.npy', the_best_loss_history)\n",
    "np.save(path_dir+'/the_best_param_history.npy', the_best_param_history)\n",
    "\n",
    "f = open(path_dir+\"/sim.cfg\", \"w+\")\n",
    "f.writelines('num: {} # The number of flowers\\n'.format(num))\n",
    "f.writelines('n_sim: {} # The number of simulation loop\\n'.format(n_sim))\n",
    "f.writelines('n_itr: {} # The number of iteration for each simulation\\n'.format(n_itr))\n",
    "f.writelines('\\n# Lambda value\\n')\n",
    "f.writelines('lambda0: {}'.format(lamda[0]))\n",
    "f.writelines('lambda1: {}'.format(lamda[1]))\n",
    "f.writelines('\\n# The boundary of the initialization value\\n')\n",
    "f.writelines('r_kp: {}\\n'.format(r_kp))\n",
    "f.writelines('r_ki: {}\\n'.format(r_ki))\n",
    "f.writelines('r_kd: {}\\n'.format(r_kd))\n",
    "f.writelines('\\n# The FPA hyperparameters\\n')\n",
    "f.writelines('s0: {}\\n'.format(s0))\n",
    "f.writelines('p_threshold: {}\\n'.format(p_threshold))\n",
    "f.writelines('alpha: {}\\n'.format(alpha))\n",
    "f.writelines('gamma: {}\\n'.format(gamma))\n",
    "f.writelines('var: {}\\n'.format(var))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda\n",
      "[30. inf]\n",
      "Parameters\n",
      "[0.47149271 0.00047589 0.05830471]\n",
      "Total loss: 15.91962556007028\n",
      "MAE: 0.6432082808794941\n",
      "MAJ: 6.15922308105343\n",
      "MSJ: 462.0152168754318\n",
      "MAUD: 0.38038863324010525\n",
      "maximum %OS: 10.009210234443081\n"
     ]
    }
   ],
   "source": [
    "print('Lambda')\n",
    "print(lamda)\n",
    "print('Parameters')\n",
    "print(global_)\n",
    "print('Total loss: {}'.format(global_loss_))\n",
    "print('MAE: {}'.format(mean_absolute_error(t0, v0, global_)))\n",
    "print('MAJ: {}'.format(mean_absolute_jerk(t0, v0, global_)))\n",
    "print('MSJ: {}'.format(mean_squared_jerk(t0, v0, global_)))\n",
    "print('MAUD: {}'.format(mean_absolute_u_dot(t0, v0, global_)))\n",
    "print('maximum %OS: {}'.format(max_percent_overshoot(t0, v0, global_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSUD: 27.819735851337157\n"
     ]
    }
   ],
   "source": [
    "print('MSUD: {}'.format(mean_squared_u_dot(t0, v0, np.array([0.56458294, 2.2533995,  0.07817718]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
